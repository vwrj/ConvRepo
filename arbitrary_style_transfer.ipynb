{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(t):\n",
    "    # t for tensor\n",
    "    \n",
    "    n_batch, n_ch, h, w = t.size()\n",
    "\n",
    "    t_view = t.view(n_batch, n_ch, h*w)\n",
    "    t_mean = t_view.mean(dim=2)\n",
    "    t_var = t_view.var(dim=2)\n",
    "\n",
    "    # broadcast\n",
    "    t_mean = t_mean.view(n_batch, n_ch, 1, 1).expand_as(t)\n",
    "    t_var = t_var.view(n_batch, n_ch, 1, 1).expand_as(t)\n",
    "    \n",
    "    return t_mean, t_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaInstanceNormalization(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    style transfer in 'feature space'. \n",
    "    Combining the content image's feature maps and style image's feature maps together, channel-wise. \n",
    "        - weighting the style feature maps much more strongly. \n",
    "        - i wonder if this could be done by just adding feature maps together, or linearly combining them in some way\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(AdaInstanceNormalization, self).__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, c, s):\n",
    "        '''\n",
    "        c - content image's feature map\n",
    "        s - style image's feature map\n",
    "        '''\n",
    "        c_mean, c_var = get_mean_var(c)\n",
    "        s_mean, s_var = get_mean_var(s)\n",
    "        \n",
    "        return s_var * ((c - c_mean) / (c_var + self.eps)) + s_mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    - construct partial VGG-19 model (through relu_4_1)\n",
    "    - will fill in weights in separate method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_norm):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # constructing partial architecture of VGG-19 (through relu_4_1)\n",
    "        conf = models.vgg.cfg['E'][:12] \n",
    "        self.features = models.vgg.make_layers(conf, batch_norm=batch_norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you initialize an nn.Module, it creates a state_dict automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(model_file, batch_norm=True):\n",
    "    '''\n",
    "    \n",
    "    initialize partial VGG-19 model and fill with pre-trained weights\n",
    "    \n",
    "    '''\n",
    "    VGG_TYPE = 'vgg19_bn' if batch_norm else 'vgg19'\n",
    "    enc = EncoderLayer(batch_norm)\n",
    "    \n",
    "    if model_file and os.path.isfile(model_file):\n",
    "        enc.load_state_dict(torch.load(model_file))\n",
    "    else:\n",
    "        vgg_weights = model_zoo.load_url(models.vgg.model_urls[VGG_TYPE])\n",
    "        w = {}\n",
    "        for key in enc.state_dict().keys():\n",
    "            w[key] = vgg_weights[key]\n",
    "        enc.load_state_dict(w)\n",
    "        if not model_file:\n",
    "            model_file = \"encoder.model\"\n",
    "        torch.save(enc.state_dict(), model_file)\n",
    "        \n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    \n",
    "    A decoder that converts a set of feature maps to an image.  \n",
    "    AdaIN layer --> decoder --> stylized image\n",
    "    \n",
    "    The reverse of VGG-19, with max-pooling replaced by upsampling. \n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        conf = [\n",
    "            (1, 256),\n",
    "            'U',\n",
    "            (3, 256),\n",
    "            'U',\n",
    "            (1, 128),\n",
    "            (1, 64),\n",
    "            'U',\n",
    "            (1, 64),\n",
    "            (1, 3)\n",
    "        ]\n",
    "        \n",
    "        self.features = self._make_layers(conf)\n",
    "        \n",
    "    def _make_layers(self, conf):\n",
    "        layers = []\n",
    "        in_channels = 512\n",
    "        for block in conf:\n",
    "            if block == 'U':\n",
    "                layers += nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "                continue\n",
    "            \n",
    "            n_layer, n_feat = block\n",
    "            for i in range(0, n_layer):\n",
    "                layers += [\n",
    "                    nn.ReflectionPad2d(1),\n",
    "                    nn.Conv2d(in_channels, n_feat, kernel_size=3, stride=1),\n",
    "                    nn.Relu()\n",
    "                ]\n",
    "                in_channels = n_feat\n",
    "        layers.pop()\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(model_file):\n",
    "    '''\n",
    "    \n",
    "    make a pre-trained partial VGG-19 network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dec = DecoderLayer()\n",
    "    if model_file and os.path.isfile(model_file):\n",
    "        dec.load_state_dict(torch.load(model_file))\n",
    "    else:\n",
    "        raise ValueError('Decoder model not found.')\n",
    "        \n",
    "    return dec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
