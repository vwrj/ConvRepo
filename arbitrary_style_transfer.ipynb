{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import get_image_backend\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import models, transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(tensor_orig, tensor_transformed, style, filename):\n",
    "    \n",
    "    def recover(t):\n",
    "        t = t.cpu().numpy()[0].transpose(1, 2, 0) * 255\n",
    "        t = t.clip(0, 255).astype(np.uint8)\n",
    "        return t\n",
    "    \n",
    "    result = Image.fromarray(recover(tensor_transformed))\n",
    "    orig = Image.fromarray(recover(tensor_orig))\n",
    "    style = Image.fromarray(recover(style))\n",
    "    \n",
    "    new_im = Image.new('RGB', (result.size[0] * 3 + 5 * 2, result.size[1]))\n",
    "    new_im.paste(orig, (0, 0))\n",
    "    new_im.paste(result, (result.size[0] + 5, 0))\n",
    "    new_im.paste(style, (result.size[0] * 2 + 10, 0))\n",
    "    new_im.save(\"/scratch/vr1059/style_transfer_debug_training/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_from_ImageNet(img):\n",
    "    '''\n",
    "    recover from ImageNet normalized rep to real img rep [0, 1]\n",
    "    '''\n",
    "    img *= torch.Tensor([0.229, 0.224, 0.225]\n",
    "                        ).view(1, 3, 1, 1).expand_as(img)\n",
    "    img += torch.Tensor([0.485, 0.456, 0.406]\n",
    "                        ).view(1, 3, 1, 1).expand_as(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    '''\n",
    "    \n",
    "    Implement perceptual (style) loss in a VGG network.\n",
    "    \n",
    "    ref:\n",
    "    https://github.com/ceshine/fast-neural-style/blob/master/style-transfer.ipynb\n",
    "    \n",
    "    input: BxCxHxW, BxCxHxW\n",
    "    output: loss type Variable\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, vgg_model):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.vgg_layers = vgg_model.features\n",
    "        \n",
    "        # use relu_1_1, 2_1, 3_1, 4_1\n",
    "        self.use_layer = set(['2', '9', '16', '29'])\n",
    "        \n",
    "    def forward(self, g, s):  \n",
    "        loss = 0\n",
    "        \n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            g, s = module(g), module(s)\n",
    "            if name in self.use_layer:\n",
    "                g_mean, g_var = get_mean_var(g)\n",
    "                s_mean, s_var = get_mean_var(s)\n",
    "                s_mean = Variable(s_mean.data, requires_grad=False)\n",
    "                s_var = Variable(s_var.data, requires_grad=False)\n",
    "                loss += F.mse_loss(g_mean, s_mean, reduction='sum') + \\\n",
    "                        F.mse_loss(g_var, s_var, reduction='sum')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(t):\n",
    "    # t for tensor\n",
    "    \n",
    "    n_batch, n_ch, h, w = t.size()\n",
    "\n",
    "    t_view = t.view(n_batch, n_ch, h*w)\n",
    "    t_mean = t_view.mean(dim=2)\n",
    "    t_var = t_view.var(dim=2)\n",
    "\n",
    "    # broadcast\n",
    "    t_mean = t_mean.view(n_batch, n_ch, 1, 1).expand_as(t)\n",
    "    t_var = t_var.view(n_batch, n_ch, 1, 1).expand_as(t)\n",
    "    \n",
    "    return t_mean, t_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaInstanceNormalization(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    style transfer in 'feature space'. \n",
    "    Combining the content image's feature maps and style image's feature maps together, channel-wise. \n",
    "        - weighting the style feature maps much more strongly. \n",
    "        - i wonder if this could be done by just adding feature maps together, or linearly combining them in some way\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(AdaInstanceNormalization, self).__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, c, s):\n",
    "        '''\n",
    "        c - content image's feature map\n",
    "        s - style image's feature map\n",
    "        '''\n",
    "        c_mean, c_var = get_mean_var(c)\n",
    "        s_mean, s_var = get_mean_var(s)\n",
    "        \n",
    "        return s_var * ((c - c_mean) / (c_var + self.eps)) + s_mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    - construct partial VGG-19 model (through relu_4_1)\n",
    "    - will fill in weights in separate method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_norm):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # constructing partial architecture of VGG-19 (through relu_4_1)\n",
    "        conf = models.vgg.cfg['E'][:12] \n",
    "        self.features = models.vgg.make_layers(conf, batch_norm=batch_norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you initialize an nn.Module, it creates a state_dict automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder(model_file, batch_norm=True):\n",
    "    '''\n",
    "    \n",
    "    initialize partial VGG-19 model and fill with pre-trained weights\n",
    "    \n",
    "    '''\n",
    "    VGG_TYPE = 'vgg19_bn' if batch_norm else 'vgg19'\n",
    "    enc = EncoderLayer(batch_norm)\n",
    "    \n",
    "    if model_file and os.path.isfile(model_file):\n",
    "        enc.load_state_dict(torch.load(model_file))\n",
    "    else:\n",
    "        vgg_weights = model_zoo.load_url(models.vgg.model_urls[VGG_TYPE])\n",
    "        w = {}\n",
    "        vgg_keys = vgg_weights.keys()\n",
    "        for key in enc.state_dict().keys():\n",
    "            if key in vgg_keys:\n",
    "                w[key] = vgg_weights[key]\n",
    "        enc.load_state_dict(w)\n",
    "        if not model_file:\n",
    "            model_file = \"encoder.model\"\n",
    "        torch.save(enc.state_dict(), model_file)\n",
    "        \n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    \n",
    "    A decoder that converts a set of feature maps to an image.  \n",
    "    AdaIN layer --> decoder --> stylized image\n",
    "    \n",
    "    The reverse of VGG-19, with max-pooling replaced by upsampling. \n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        conf = [\n",
    "            (1, 256),\n",
    "            'U',\n",
    "            (3, 256),\n",
    "            'U',\n",
    "            (1, 128),\n",
    "            (1, 64),\n",
    "            'U',\n",
    "            (1, 64),\n",
    "            (1, 3)\n",
    "        ]\n",
    "        \n",
    "        self.features = self._make_layers(conf)\n",
    "        \n",
    "    def _make_layers(self, conf):\n",
    "        layers = []\n",
    "        in_channels = 512\n",
    "        for block in conf:\n",
    "            if block == 'U':\n",
    "                layers += [nn.Upsample(scale_factor=2, mode='bilinear')]\n",
    "                continue\n",
    "            \n",
    "            n_layer, n_feat = block\n",
    "            for i in range(0, n_layer):\n",
    "                layers += [\n",
    "                    nn.ReflectionPad2d(1),\n",
    "                    nn.Conv2d(in_channels, n_feat, kernel_size=3, stride=1),\n",
    "                    nn.ReLU()\n",
    "                ]\n",
    "                in_channels = n_feat\n",
    "        layers.pop()\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(model_file):\n",
    "    '''\n",
    "    \n",
    "    make a pre-trained partial VGG-19 network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dec = DecoderLayer()\n",
    "    if model_file and os.path.isfile(model_file):\n",
    "        dec.load_state_dict(torch.load(model_file))\n",
    "    else:\n",
    "        raise ValueError('Decoder model not found.')\n",
    "        \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    epochs = 100\n",
    "    batch_size = 8\n",
    "    image_size = 256\n",
    "    content_folder = '/scratch/vr1059/coco'\n",
    "    style_folder = '/scratch/vr1059/painter_by_numbers'\n",
    "    model_encoder = ''\n",
    "    lr = 1e-4\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.RandomCrop(args.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    # VGG was trained on images normalized with those statistics\n",
    "])\n",
    "\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.RandomCrop(args.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    # VGG was trained on images normalized with those statistics\n",
    "])\n",
    "\n",
    "image_transform_nocrop = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (string): Path to image directory. \n",
    "            transform (callable, optional): (optional) transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.fnames = [f for f in listdir(path)]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.fnames[idx]\n",
    "        image = pil_loader(self.path + img_name)\n",
    "        sample = self.transform(image)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content and style loader\n",
    "content_train_dataset = ImageDataset(args.content_folder + '/train/', image_transform)\n",
    "content_train_loader = DataLoader(\n",
    "    content_train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "content_validation_dataset = ImageDataset(args.content_folder + '/val/', image_transform)\n",
    "content_validation_loader = DataLoader(\n",
    "    content_validation_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_train_dataset = ImageDataset(args.style_folder + '/train/', image_transform)\n",
    "style_train_loader = DataLoader(\n",
    "    style_train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "style_validation_dataset = ImageDataset(args.style_folder + '/test/', image_transform)\n",
    "style_validation_loader = DataLoader(\n",
    "    style_validation_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = make_encoder('encoder.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaIN = AdaInstanceNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = DecoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual_loss = PerceptualLoss(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('0', Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('1', BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('2', ReLU(inplace)), ('3', Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('4', BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('5', ReLU(inplace)), ('6', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)), ('7', Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('8', BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('9', ReLU(inplace)), ('10', Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('11', BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('12', ReLU(inplace)), ('13', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)), ('14', Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('15', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('16', ReLU(inplace)), ('17', Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('18', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('19', ReLU(inplace)), ('20', Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('21', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('22', ReLU(inplace)), ('23', Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('24', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('25', ReLU(inplace)), ('26', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)), ('27', Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('28', BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), ('29', ReLU(inplace))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.features._modules.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in dec.parameters():\n",
    "    if param.dim() < 2:\n",
    "        continue\n",
    "    nn.init.kaiming_normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_SEED = 1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b4deaf6f850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.to(device)\n",
    "adaIN.to(device)\n",
    "dec.to(device)\n",
    "perceptual_loss.to(device)\n",
    "torch.cuda.manual_seed(TORCH_SEED)\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "optimizer = Adam(dec.parameters(), args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.train()\n",
    "enc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DEBUG_IMAGE_PER_NBATCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    enc.train()\n",
    "    \n",
    "    loader = zip(content_train_loader, style_train_loader)\n",
    "    \n",
    "    avg_closs = avg_sloss = avg_loss = 0\n",
    "    \n",
    "    n_samples = 0\n",
    "    \n",
    "    for (i, (xx, ss)) in enumerate(loader):\n",
    "        \n",
    "        x = Variable(xx).to(device)\n",
    "        s = Variable(ss).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        c_feat, s_feat = enc(x), enc(s)\n",
    "        \n",
    "        t = adaIN(c_feat, s_feat)\n",
    "        \n",
    "        gen_img = dec(t)\n",
    "        \n",
    "        ft = enc(gen_img)\n",
    "        \n",
    "        content_loss = F.mse_loss(ft, t, reduction='sum')\n",
    "        style_loss = perceptual_loss(gen_img, s.expand_as(gen_img))\n",
    "        loss = content_loss + 0.1 * style_loss\n",
    "                \n",
    "        batch_closs, batch_sloss, batch_loss = content_loss, style_loss, loss \n",
    "        \n",
    "        n_samples += len(xx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % SAVE_DEBUG_IMAGE_PER_NBATCH == 0:\n",
    "            print(\"Train Epoch {}: Batch {} content: {} style: {} loss: {}\".format(epoch, i, batch_closs/len(xx), batch_sloss/len(xx), batch_loss/len(xx)))\n",
    "            \n",
    "            save_img(recover_from_ImageNet(x.data.cpu()), recover_from_ImageNet(gen_img.data.cpu()), recover_from_ImageNet(s.data.cpu()), \"debug_train_{}_{}.png\".format(epoch, i))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/vr1059/py3.6.3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Batch 0 content: 14916.53515625 style: 9951.9365234375 loss: 15911.728515625\n",
      "Train Epoch 0: Batch 20 content: 10981.8134765625 style: 9752.4482421875 loss: 11957.05859375\n",
      "Train Epoch 0: Batch 40 content: 11706.8330078125 style: 10138.380859375 loss: 12720.6708984375\n",
      "Train Epoch 0: Batch 60 content: 15678.97265625 style: 10924.1494140625 loss: 16771.38671875\n",
      "Train Epoch 0: Batch 80 content: 10249.3837890625 style: 10984.7080078125 loss: 11347.8544921875\n",
      "Train Epoch 0: Batch 100 content: 12369.9375 style: 11034.802734375 loss: 13473.41796875\n",
      "Train Epoch 0: Batch 120 content: 14503.009765625 style: 7073.50048828125 loss: 15210.359375\n",
      "Train Epoch 0: Batch 140 content: 12132.37890625 style: 7628.33984375 loss: 12895.212890625\n",
      "Train Epoch 0: Batch 160 content: 11995.4453125 style: 8303.0244140625 loss: 12825.748046875\n",
      "Train Epoch 0: Batch 180 content: 9888.6103515625 style: 10764.60546875 loss: 10965.0712890625\n",
      "Train Epoch 0: Batch 200 content: 12642.94140625 style: 5868.79931640625 loss: 13229.8212890625\n",
      "Train Epoch 0: Batch 220 content: 14318.486328125 style: 5193.09619140625 loss: 14837.7958984375\n",
      "Train Epoch 0: Batch 240 content: 21778.4453125 style: 11322.224609375 loss: 22910.66796875\n",
      "Train Epoch 0: Batch 260 content: 11082.087890625 style: 12073.0576171875 loss: 12289.3935546875\n",
      "Train Epoch 0: Batch 280 content: 15466.3056640625 style: 5881.5205078125 loss: 16054.4580078125\n",
      "Train Epoch 0: Batch 300 content: 13279.16796875 style: 12666.0234375 loss: 14545.7705078125\n",
      "Train Epoch 0: Batch 320 content: 12262.431640625 style: 7051.72802734375 loss: 12967.6044921875\n",
      "Train Epoch 0: Batch 340 content: 11988.685546875 style: 6752.498046875 loss: 12663.935546875\n",
      "Train Epoch 0: Batch 360 content: 11245.28515625 style: 10021.66015625 loss: 12247.451171875\n",
      "Train Epoch 0: Batch 380 content: 14337.1259765625 style: 7904.62744140625 loss: 15127.5888671875\n",
      "Train Epoch 0: Batch 400 content: 20893.02734375 style: 4928.56396484375 loss: 21385.8828125\n",
      "Train Epoch 0: Batch 420 content: 11492.3203125 style: 5703.74755859375 loss: 12062.6953125\n",
      "Train Epoch 0: Batch 440 content: 11328.490234375 style: 7450.25390625 loss: 12073.515625\n",
      "Train Epoch 0: Batch 460 content: 43841.48046875 style: 6184.61767578125 loss: 44459.94140625\n",
      "Train Epoch 0: Batch 480 content: 10377.18359375 style: 5297.9716796875 loss: 10906.98046875\n",
      "Train Epoch 0: Batch 500 content: 10239.935546875 style: 4903.34375 loss: 10730.26953125\n",
      "Train Epoch 0: Batch 520 content: 21724.134765625 style: 9467.244140625 loss: 22670.859375\n",
      "Train Epoch 0: Batch 540 content: 30256.42578125 style: 5647.08154296875 loss: 30821.134765625\n",
      "Train Epoch 0: Batch 560 content: 10076.958984375 style: 8073.11181640625 loss: 10884.2705078125\n",
      "Train Epoch 0: Batch 580 content: 9596.8203125 style: 6949.74169921875 loss: 10291.794921875\n",
      "Train Epoch 0: Batch 600 content: 10188.578125 style: 5551.05908203125 loss: 10743.68359375\n",
      "Train Epoch 0: Batch 620 content: 9510.3271484375 style: 7819.74267578125 loss: 10292.3017578125\n",
      "Train Epoch 0: Batch 640 content: 8944.955078125 style: 6211.41552734375 loss: 9566.0966796875\n",
      "Train Epoch 0: Batch 660 content: 9125.640625 style: 6157.2744140625 loss: 9741.3681640625\n",
      "Train Epoch 0: Batch 680 content: 9622.818359375 style: 6904.5625 loss: 10313.2744140625\n",
      "Train Epoch 0: Batch 700 content: 10983.427734375 style: 5557.923828125 loss: 11539.2197265625\n",
      "Train Epoch 0: Batch 720 content: 9255.1865234375 style: 5669.16552734375 loss: 9822.103515625\n",
      "Train Epoch 0: Batch 740 content: 9526.6767578125 style: 7140.00244140625 loss: 10240.6767578125\n",
      "Train Epoch 0: Batch 760 content: 14054.759765625 style: 3502.2861328125 loss: 14404.98828125\n",
      "Train Epoch 0: Batch 780 content: 20003.962890625 style: 6143.07275390625 loss: 20618.26953125\n",
      "Train Epoch 0: Batch 860 content: 10684.2587890625 style: 5283.2939453125 loss: 11212.587890625\n",
      "Train Epoch 0: Batch 880 content: 8655.4130859375 style: 5040.76513671875 loss: 9159.4892578125\n",
      "Train Epoch 0: Batch 900 content: 7057.05712890625 style: 4596.8154296875 loss: 7516.73876953125\n",
      "Train Epoch 0: Batch 920 content: 13413.41796875 style: 6425.92724609375 loss: 14056.0107421875\n",
      "Train Epoch 0: Batch 940 content: 10894.80078125 style: 4970.25146484375 loss: 11391.826171875\n",
      "Train Epoch 0: Batch 960 content: 8018.439453125 style: 5622.94970703125 loss: 8580.734375\n",
      "Train Epoch 0: Batch 980 content: 12357.0068359375 style: 5858.560546875 loss: 12942.86328125\n",
      "Train Epoch 0: Batch 1000 content: 8113.57958984375 style: 4054.542236328125 loss: 8519.0341796875\n",
      "Train Epoch 0: Batch 1020 content: 13514.314453125 style: 4500.134765625 loss: 13964.328125\n",
      "Train Epoch 0: Batch 1040 content: 7092.4482421875 style: 3916.0556640625 loss: 7484.0537109375\n",
      "Train Epoch 0: Batch 1060 content: 7312.4482421875 style: 3607.725341796875 loss: 7673.220703125\n",
      "Train Epoch 0: Batch 1080 content: 7577.8388671875 style: 5215.31982421875 loss: 8099.37109375\n",
      "Train Epoch 0: Batch 1100 content: 8774.0068359375 style: 5332.92041015625 loss: 9307.298828125\n",
      "Train Epoch 0: Batch 1120 content: 7292.9501953125 style: 3792.259521484375 loss: 7672.17626953125\n",
      "Train Epoch 0: Batch 1140 content: 8362.6455078125 style: 5197.94384765625 loss: 8882.439453125\n",
      "Train Epoch 0: Batch 1160 content: 7620.271484375 style: 5434.35546875 loss: 8163.70703125\n",
      "Train Epoch 0: Batch 1180 content: 8348.826171875 style: 4197.9267578125 loss: 8768.619140625\n",
      "Train Epoch 0: Batch 1200 content: 8481.3447265625 style: 5007.5556640625 loss: 8982.1005859375\n",
      "Train Epoch 0: Batch 1220 content: 9082.251953125 style: 3080.630859375 loss: 9390.3154296875\n",
      "Train Epoch 0: Batch 1240 content: 8700.611328125 style: 5403.5361328125 loss: 9240.96484375\n",
      "Train Epoch 0: Batch 1260 content: 14478.8623046875 style: 4526.2099609375 loss: 14931.4833984375\n",
      "Train Epoch 0: Batch 1280 content: 11544.7666015625 style: 6161.96240234375 loss: 12160.962890625\n",
      "Train Epoch 0: Batch 1300 content: 6430.7646484375 style: 5089.67333984375 loss: 6939.73193359375\n",
      "Train Epoch 0: Batch 1320 content: 12293.3037109375 style: 5477.404296875 loss: 12841.0439453125\n",
      "Train Epoch 0: Batch 1340 content: 10585.33984375 style: 6564.97509765625 loss: 11241.8369140625\n",
      "Train Epoch 0: Batch 1360 content: 9818.71875 style: 4915.48876953125 loss: 10310.267578125\n",
      "Train Epoch 0: Batch 1380 content: 8890.541015625 style: 4352.5234375 loss: 9325.79296875\n",
      "Train Epoch 0: Batch 1400 content: 8325.1025390625 style: 5085.01171875 loss: 8833.603515625\n",
      "Train Epoch 0: Batch 1420 content: 11895.771484375 style: 4970.26123046875 loss: 12392.7978515625\n",
      "Train Epoch 0: Batch 1440 content: 9571.3779296875 style: 6540.54736328125 loss: 10225.4326171875\n",
      "Train Epoch 0: Batch 1460 content: 24446.27734375 style: 7699.77197265625 loss: 25216.25390625\n",
      "Train Epoch 0: Batch 1480 content: 8970.517578125 style: 4069.071044921875 loss: 9377.4248046875\n",
      "Train Epoch 0: Batch 1500 content: 8252.0224609375 style: 3767.7958984375 loss: 8628.8017578125\n",
      "Train Epoch 0: Batch 1520 content: 48047.6796875 style: 6121.0341796875 loss: 48659.78125\n",
      "Train Epoch 0: Batch 1540 content: 10917.984375 style: 5145.1552734375 loss: 11432.5\n",
      "Train Epoch 0: Batch 1560 content: 15806.953125 style: 4696.490234375 loss: 16276.6025390625\n",
      "Train Epoch 0: Batch 1580 content: 7014.8369140625 style: 4299.09619140625 loss: 7444.74658203125\n",
      "Train Epoch 0: Batch 1600 content: 6575.75341796875 style: 4723.9873046875 loss: 7048.15234375\n",
      "Train Epoch 0: Batch 1620 content: 7764.3212890625 style: 3073.176025390625 loss: 8071.638671875\n",
      "Train Epoch 0: Batch 1640 content: 10024.48828125 style: 4873.38037109375 loss: 10511.826171875\n",
      "Train Epoch 0: Batch 1660 content: 7758.423828125 style: 5044.8701171875 loss: 8262.9111328125\n",
      "Train Epoch 0: Batch 1680 content: 7137.953125 style: 3489.47607421875 loss: 7486.90087890625\n",
      "Train Epoch 0: Batch 1700 content: 18192.84375 style: 6554.8203125 loss: 18848.326171875\n",
      "Train Epoch 0: Batch 1720 content: 9422.67578125 style: 4419.20556640625 loss: 9864.5966796875\n",
      "Train Epoch 0: Batch 1740 content: 6911.2744140625 style: 4117.2587890625 loss: 7323.00048828125\n",
      "Train Epoch 0: Batch 1760 content: 7672.95556640625 style: 5265.92431640625 loss: 8199.5478515625\n",
      "Train Epoch 0: Batch 1780 content: 7918.775390625 style: 4637.8818359375 loss: 8382.5634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Batch 1800 content: 13786.5634765625 style: 3960.9658203125 loss: 14182.66015625\n",
      "Train Epoch 0: Batch 1820 content: 9505.1337890625 style: 3376.942626953125 loss: 9842.828125\n",
      "Train Epoch 0: Batch 1840 content: 7570.62158203125 style: 3339.636474609375 loss: 7904.58544921875\n",
      "Train Epoch 0: Batch 1860 content: 14534.1669921875 style: 4977.818359375 loss: 15031.94921875\n",
      "Train Epoch 0: Batch 1880 content: 7262.0478515625 style: 3040.476318359375 loss: 7566.095703125\n",
      "Train Epoch 0: Batch 1900 content: 8573.0673828125 style: 5945.01513671875 loss: 9167.5693359375\n",
      "Train Epoch 0: Batch 1920 content: 18840.49609375 style: 4653.5244140625 loss: 19305.84765625\n",
      "Train Epoch 0: Batch 1940 content: 9218.375 style: 4289.42822265625 loss: 9647.3173828125\n",
      "Train Epoch 0: Batch 1960 content: 7508.49658203125 style: 3853.922119140625 loss: 7893.888671875\n",
      "Train Epoch 0: Batch 1980 content: 26678.8046875 style: 5123.2724609375 loss: 27191.1328125\n",
      "Train Epoch 0: Batch 2000 content: 8509.9541015625 style: 5086.10400390625 loss: 9018.564453125\n",
      "Train Epoch 0: Batch 2020 content: 12934.3359375 style: 4152.0859375 loss: 13349.544921875\n",
      "Train Epoch 0: Batch 2040 content: 22578.51953125 style: 5716.60205078125 loss: 23150.1796875\n",
      "Train Epoch 0: Batch 2060 content: 7600.0234375 style: 2533.664306640625 loss: 7853.3896484375\n",
      "Train Epoch 0: Batch 2080 content: 8232.134765625 style: 5708.666015625 loss: 8803.0009765625\n",
      "Train Epoch 0: Batch 2100 content: 9345.9423828125 style: 4335.26318359375 loss: 9779.46875\n",
      "Train Epoch 0: Batch 2120 content: 7634.46240234375 style: 4828.15966796875 loss: 8117.2783203125\n",
      "Train Epoch 0: Batch 2140 content: 7929.35400390625 style: 7296.2666015625 loss: 8658.98046875\n",
      "Train Epoch 0: Batch 2160 content: 12828.580078125 style: 6014.72705078125 loss: 13430.052734375\n",
      "Train Epoch 0: Batch 2180 content: 15107.744140625 style: 6673.24658203125 loss: 15775.068359375\n",
      "Train Epoch 0: Batch 2200 content: 9826.7509765625 style: 3994.31982421875 loss: 10226.1826171875\n",
      "Train Epoch 0: Batch 2220 content: 6504.02734375 style: 3725.43896484375 loss: 6876.5712890625\n",
      "Train Epoch 0: Batch 2240 content: 9792.17578125 style: 4497.60791015625 loss: 10241.9365234375\n",
      "Train Epoch 0: Batch 2260 content: 8566.208984375 style: 2902.828125 loss: 8856.4921875\n",
      "Train Epoch 0: Batch 2280 content: 9643.859375 style: 3324.647216796875 loss: 9976.32421875\n",
      "Train Epoch 0: Batch 2300 content: 11354.251953125 style: 2967.442138671875 loss: 11650.99609375\n",
      "Train Epoch 0: Batch 2320 content: 8475.7578125 style: 2534.9462890625 loss: 8729.251953125\n",
      "Train Epoch 0: Batch 2340 content: 7142.6591796875 style: 5229.0283203125 loss: 7665.56201171875\n",
      "Train Epoch 0: Batch 2360 content: 7839.92529296875 style: 6234.1357421875 loss: 8463.3388671875\n",
      "Train Epoch 0: Batch 2380 content: 8924.951171875 style: 3190.32861328125 loss: 9243.984375\n",
      "Train Epoch 0: Batch 2400 content: 7030.615234375 style: 4154.35009765625 loss: 7446.05029296875\n",
      "Train Epoch 0: Batch 2420 content: 8442.9609375 style: 3179.3623046875 loss: 8760.8974609375\n",
      "Train Epoch 0: Batch 2440 content: 7626.771484375 style: 2761.319580078125 loss: 7902.9033203125\n",
      "Train Epoch 0: Batch 2460 content: 7360.3818359375 style: 4759.55419921875 loss: 7836.33740234375\n",
      "Train Epoch 0: Batch 2480 content: 6282.2568359375 style: 2780.99609375 loss: 6560.3564453125\n",
      "Train Epoch 0: Batch 2500 content: 7842.8857421875 style: 3046.23388671875 loss: 8147.50927734375\n",
      "Train Epoch 0: Batch 2520 content: 9902.197265625 style: 3514.310546875 loss: 10253.6279296875\n",
      "Train Epoch 0: Batch 2540 content: 9314.525390625 style: 3631.32421875 loss: 9677.658203125\n",
      "Train Epoch 0: Batch 2560 content: 9367.046875 style: 4212.546875 loss: 9788.3017578125\n",
      "Train Epoch 0: Batch 2580 content: 9381.9384765625 style: 3675.28466796875 loss: 9749.466796875\n",
      "Train Epoch 0: Batch 2600 content: 12150.8125 style: 4380.8291015625 loss: 12588.8955078125\n",
      "Train Epoch 0: Batch 2620 content: 19258.38671875 style: 4714.18994140625 loss: 19729.806640625\n",
      "Train Epoch 0: Batch 2640 content: 10280.80859375 style: 5287.287109375 loss: 10809.537109375\n",
      "Train Epoch 0: Batch 2660 content: 9489.578125 style: 3215.649169921875 loss: 9811.142578125\n",
      "Train Epoch 0: Batch 2680 content: 6753.38330078125 style: 3122.345947265625 loss: 7065.61767578125\n",
      "Train Epoch 0: Batch 2700 content: 6368.1396484375 style: 2594.116943359375 loss: 6627.55126953125\n",
      "Train Epoch 0: Batch 2720 content: 7903.70263671875 style: 3366.610107421875 loss: 8240.36328125\n",
      "Train Epoch 0: Batch 2740 content: 8729.431640625 style: 3576.332275390625 loss: 9087.064453125\n",
      "Train Epoch 0: Batch 2760 content: 12227.595703125 style: 3839.347412109375 loss: 12611.5302734375\n",
      "Train Epoch 0: Batch 2780 content: 6526.2353515625 style: 3964.702880859375 loss: 6922.70556640625\n",
      "Train Epoch 0: Batch 2800 content: 28660.083984375 style: 3503.02880859375 loss: 29010.38671875\n",
      "Train Epoch 0: Batch 2820 content: 6488.61865234375 style: 2743.715087890625 loss: 6762.990234375\n",
      "Train Epoch 0: Batch 2840 content: 9372.630859375 style: 4285.65869140625 loss: 9801.1962890625\n",
      "Train Epoch 0: Batch 2860 content: 9448.505859375 style: 3635.291748046875 loss: 9812.03515625\n",
      "Train Epoch 0: Batch 2880 content: 8518.791015625 style: 3393.4951171875 loss: 8858.140625\n",
      "Train Epoch 0: Batch 2900 content: 9421.28515625 style: 3899.789306640625 loss: 9811.263671875\n",
      "Train Epoch 0: Batch 2920 content: 8230.34765625 style: 3000.94091796875 loss: 8530.44140625\n",
      "Train Epoch 0: Batch 2940 content: 9302.029296875 style: 7298.87841796875 loss: 10031.9169921875\n",
      "Train Epoch 0: Batch 2960 content: 9313.63671875 style: 3707.725830078125 loss: 9684.4091796875\n",
      "Train Epoch 0: Batch 2980 content: 6432.6962890625 style: 4949.96484375 loss: 6927.69287109375\n",
      "Train Epoch 0: Batch 3000 content: 17454.8984375 style: 3264.76416015625 loss: 17781.375\n",
      "Train Epoch 0: Batch 3020 content: 7247.78955078125 style: 3354.6357421875 loss: 7583.2529296875\n",
      "Train Epoch 0: Batch 3040 content: 7352.54736328125 style: 3399.8876953125 loss: 7692.5361328125\n",
      "Train Epoch 0: Batch 3060 content: 8555.501953125 style: 2962.481689453125 loss: 8851.75\n",
      "Train Epoch 0: Batch 3080 content: 9046.068359375 style: 3038.4365234375 loss: 9349.912109375\n",
      "Train Epoch 0: Batch 3100 content: 26525.11328125 style: 4199.662109375 loss: 26945.080078125\n",
      "Train Epoch 0: Batch 3120 content: 8048.279296875 style: 2212.57275390625 loss: 8269.5361328125\n",
      "Train Epoch 0: Batch 3140 content: 7107.98046875 style: 2457.479248046875 loss: 7353.728515625\n",
      "Train Epoch 0: Batch 3160 content: 7565.134765625 style: 4278.73583984375 loss: 7993.00830078125\n",
      "Train Epoch 0: Batch 3180 content: 18660.6484375 style: 5291.0126953125 loss: 19189.75\n",
      "Train Epoch 0: Batch 3200 content: 5828.60107421875 style: 3034.36181640625 loss: 6132.037109375\n",
      "Train Epoch 0: Batch 3220 content: 6796.61767578125 style: 3909.31689453125 loss: 7187.54931640625\n",
      "Train Epoch 0: Batch 3240 content: 5698.130859375 style: 4547.3876953125 loss: 6152.86962890625\n",
      "Train Epoch 0: Batch 3260 content: 7072.28515625 style: 3596.575439453125 loss: 7431.94287109375\n",
      "Train Epoch 0: Batch 3280 content: 12408.412109375 style: 3758.336181640625 loss: 12784.24609375\n",
      "Train Epoch 0: Batch 3300 content: 6954.8896484375 style: 3343.423583984375 loss: 7289.23193359375\n",
      "Train Epoch 0: Batch 3320 content: 6428.0595703125 style: 3655.433837890625 loss: 6793.60302734375\n",
      "Train Epoch 0: Batch 3340 content: 29420.671875 style: 4423.80615234375 loss: 29863.052734375\n",
      "Train Epoch 0: Batch 3360 content: 7980.4296875 style: 3041.8369140625 loss: 8284.61328125\n",
      "Train Epoch 0: Batch 3380 content: 6421.12890625 style: 2971.147216796875 loss: 6718.24365234375\n",
      "Train Epoch 0: Batch 3400 content: 13878.90234375 style: 4054.644287109375 loss: 14284.3671875\n",
      "Train Epoch 0: Batch 3420 content: 16130.826171875 style: 3336.00634765625 loss: 16464.427734375\n",
      "Train Epoch 0: Batch 3440 content: 8090.6865234375 style: 2803.766845703125 loss: 8371.0634765625\n",
      "Train Epoch 0: Batch 3460 content: 6233.0966796875 style: 3135.054931640625 loss: 6546.60205078125\n",
      "Train Epoch 0: Batch 3560 content: 9906.765625 style: 3272.6318359375 loss: 10234.029296875\n",
      "Train Epoch 0: Batch 3580 content: 14384.5234375 style: 3845.74951171875 loss: 14769.0986328125\n",
      "Train Epoch 0: Batch 3600 content: 8760.32421875 style: 3584.59326171875 loss: 9118.783203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Batch 3620 content: 5995.4716796875 style: 2764.83349609375 loss: 6271.955078125\n",
      "Train Epoch 0: Batch 3640 content: 5603.1044921875 style: 2655.77978515625 loss: 5868.6826171875\n",
      "Train Epoch 0: Batch 3660 content: 16665.7421875 style: 4061.98291015625 loss: 17071.94140625\n",
      "Train Epoch 0: Batch 3680 content: 6597.08056640625 style: 2648.440673828125 loss: 6861.9248046875\n",
      "Train Epoch 0: Batch 3700 content: 6716.962890625 style: 2760.108154296875 loss: 6992.9736328125\n",
      "Train Epoch 0: Batch 3720 content: 6024.44384765625 style: 3746.97265625 loss: 6399.14111328125\n",
      "Train Epoch 0: Batch 3740 content: 7473.87060546875 style: 3447.341552734375 loss: 7818.60498046875\n",
      "Train Epoch 0: Batch 3760 content: 20901.7265625 style: 3143.1181640625 loss: 21216.0390625\n",
      "Train Epoch 0: Batch 3780 content: 5857.458984375 style: 2985.51513671875 loss: 6156.0107421875\n",
      "Train Epoch 0: Batch 3800 content: 10349.923828125 style: 3917.008544921875 loss: 10741.625\n",
      "Train Epoch 0: Batch 3820 content: 9064.396484375 style: 3543.07373046875 loss: 9418.7041015625\n",
      "Train Epoch 0: Batch 3840 content: 9266.4765625 style: 4146.9140625 loss: 9681.16796875\n",
      "Train Epoch 0: Batch 3860 content: 8485.376953125 style: 2871.9638671875 loss: 8772.5732421875\n",
      "Train Epoch 0: Batch 3880 content: 23478.720703125 style: 4192.27978515625 loss: 23897.94921875\n",
      "Train Epoch 0: Batch 3900 content: 9012.890625 style: 3828.963623046875 loss: 9395.787109375\n",
      "Train Epoch 0: Batch 3920 content: 11207.013671875 style: 4608.80615234375 loss: 11667.89453125\n",
      "Train Epoch 0: Batch 3940 content: 19144.216796875 style: 3384.6171875 loss: 19482.677734375\n",
      "Train Epoch 0: Batch 3960 content: 8256.365234375 style: 2978.8828125 loss: 8554.25390625\n",
      "Train Epoch 0: Batch 3980 content: 9239.4423828125 style: 4749.130859375 loss: 9714.35546875\n",
      "Train Epoch 0: Batch 4000 content: 7684.04052734375 style: 3069.02685546875 loss: 7990.943359375\n",
      "Train Epoch 0: Batch 4020 content: 5785.9462890625 style: 6214.16796875 loss: 6407.36328125\n",
      "Train Epoch 0: Batch 4040 content: 7876.8740234375 style: 4194.7080078125 loss: 8296.3447265625\n",
      "Train Epoch 0: Batch 4060 content: 5202.4111328125 style: 2512.939208984375 loss: 5453.705078125\n",
      "Train Epoch 0: Batch 4080 content: 6717.77734375 style: 2810.437744140625 loss: 6998.8212890625\n",
      "Train Epoch 0: Batch 4100 content: 8604.412109375 style: 2426.78076171875 loss: 8847.08984375\n",
      "Train Epoch 0: Batch 4120 content: 5711.3818359375 style: 2248.557373046875 loss: 5936.23779296875\n",
      "Train Epoch 0: Batch 4140 content: 7658.5439453125 style: 2792.74853515625 loss: 7937.81884765625\n",
      "Train Epoch 0: Batch 4160 content: 12777.7099609375 style: 4149.15283203125 loss: 13192.625\n",
      "Train Epoch 0: Batch 4180 content: 8006.5068359375 style: 2930.82763671875 loss: 8299.58984375\n",
      "Train Epoch 0: Batch 4200 content: 9114.4296875 style: 2769.975341796875 loss: 9391.4267578125\n",
      "Train Epoch 0: Batch 4220 content: 13919.623046875 style: 2799.361083984375 loss: 14199.5595703125\n",
      "Train Epoch 0: Batch 4240 content: 18740.609375 style: 3554.362548828125 loss: 19096.044921875\n",
      "Train Epoch 0: Batch 4260 content: 8388.2236328125 style: 4326.390625 loss: 8820.8623046875\n",
      "Train Epoch 0: Batch 4280 content: 7829.61962890625 style: 4617.55859375 loss: 8291.375\n",
      "Train Epoch 0: Batch 4300 content: 27700.453125 style: 3223.95751953125 loss: 28022.849609375\n",
      "Train Epoch 0: Batch 4320 content: 5843.314453125 style: 2920.210693359375 loss: 6135.33544921875\n",
      "Train Epoch 0: Batch 4340 content: 6023.68896484375 style: 3302.36376953125 loss: 6353.92529296875\n",
      "Train Epoch 0: Batch 4360 content: 10112.599609375 style: 3266.292724609375 loss: 10439.228515625\n",
      "Train Epoch 0: Batch 4380 content: 6605.8486328125 style: 3114.285400390625 loss: 6917.27734375\n",
      "Train Epoch 0: Batch 4400 content: 8787.8916015625 style: 2460.660400390625 loss: 9033.9580078125\n",
      "Train Epoch 0: Batch 4420 content: 7256.2021484375 style: 4039.62646484375 loss: 7660.1650390625\n",
      "Train Epoch 0: Batch 4440 content: 16376.06640625 style: 4962.40771484375 loss: 16872.306640625\n",
      "Train Epoch 0: Batch 4460 content: 6994.99560546875 style: 3475.25341796875 loss: 7342.52099609375\n",
      "Train Epoch 0: Batch 4480 content: 9337.5615234375 style: 3137.47705078125 loss: 9651.3095703125\n",
      "Train Epoch 0: Batch 4500 content: 6561.3115234375 style: 4362.1728515625 loss: 6997.52880859375\n",
      "Train Epoch 0: Batch 4520 content: 9533.94140625 style: 2554.35791015625 loss: 9789.376953125\n",
      "Train Epoch 0: Batch 4540 content: 9170.8515625 style: 3668.50341796875 loss: 9537.7021484375\n",
      "Train Epoch 0: Batch 4560 content: 5885.568359375 style: 3118.454833984375 loss: 6197.4140625\n",
      "Train Epoch 0: Batch 4580 content: 7155.5830078125 style: 4207.00927734375 loss: 7576.2841796875\n",
      "Train Epoch 0: Batch 4600 content: 13234.869140625 style: 3471.373046875 loss: 13582.0068359375\n",
      "Train Epoch 0: Batch 4620 content: 11738.435546875 style: 3261.0029296875 loss: 12064.5361328125\n",
      "Train Epoch 0: Batch 4640 content: 8005.599609375 style: 5022.9619140625 loss: 8507.8955078125\n",
      "Train Epoch 0: Batch 4660 content: 9195.236328125 style: 2780.478759765625 loss: 9473.2841796875\n",
      "Train Epoch 0: Batch 4680 content: 10511.8505859375 style: 4157.8837890625 loss: 10927.638671875\n",
      "Train Epoch 0: Batch 4700 content: 9339.13671875 style: 3797.75537109375 loss: 9718.912109375\n",
      "Train Epoch 0: Batch 4720 content: 5920.841796875 style: 3151.2099609375 loss: 6235.962890625\n",
      "Train Epoch 0: Batch 4740 content: 7455.7001953125 style: 4066.699951171875 loss: 7862.3701171875\n",
      "Train Epoch 0: Batch 4760 content: 8403.19140625 style: 3473.528564453125 loss: 8750.5439453125\n",
      "Train Epoch 0: Batch 4780 content: 6674.98046875 style: 2986.197021484375 loss: 6973.60009765625\n",
      "Train Epoch 0: Batch 4800 content: 5149.7626953125 style: 1545.8201904296875 loss: 5304.3447265625\n",
      "Train Epoch 0: Batch 4820 content: 6161.392578125 style: 2427.141845703125 loss: 6404.10693359375\n",
      "Train Epoch 0: Batch 4840 content: 7094.748046875 style: 2597.0263671875 loss: 7354.45068359375\n",
      "Train Epoch 0: Batch 4860 content: 8925.041015625 style: 4318.30224609375 loss: 9356.87109375\n",
      "Train Epoch 0: Batch 4880 content: 9257.94140625 style: 3716.943115234375 loss: 9629.6357421875\n",
      "Train Epoch 0: Batch 4900 content: 8582.439453125 style: 2710.173828125 loss: 8853.45703125\n",
      "Train Epoch 0: Batch 4920 content: 7805.12060546875 style: 2020.5916748046875 loss: 8007.1796875\n",
      "Train Epoch 0: Batch 4940 content: 9835.693359375 style: 3825.644287109375 loss: 10218.2578125\n",
      "Train Epoch 0: Batch 4960 content: 5862.8291015625 style: 3152.359375 loss: 6178.06494140625\n",
      "Train Epoch 0: Batch 4980 content: 7520.28125 style: 2913.022705078125 loss: 7811.58349609375\n",
      "Train Epoch 0: Batch 5000 content: 6465.2509765625 style: 4901.87841796875 loss: 6955.43896484375\n",
      "Train Epoch 0: Batch 5020 content: 6239.1201171875 style: 2879.1015625 loss: 6527.0302734375\n",
      "Train Epoch 0: Batch 5040 content: 7630.72119140625 style: 2973.506591796875 loss: 7928.07177734375\n",
      "Train Epoch 0: Batch 5060 content: 5756.130859375 style: 3158.2783203125 loss: 6071.95849609375\n",
      "Train Epoch 0: Batch 5080 content: 6756.38134765625 style: 4364.80859375 loss: 7192.8623046875\n",
      "Train Epoch 0: Batch 5100 content: 6468.146484375 style: 3347.598876953125 loss: 6802.90625\n",
      "Train Epoch 0: Batch 5120 content: 6633.37158203125 style: 3134.5556640625 loss: 6946.8271484375\n",
      "Train Epoch 0: Batch 5140 content: 7247.9658203125 style: 3280.474853515625 loss: 7576.01318359375\n",
      "Train Epoch 0: Batch 5160 content: 7154.88671875 style: 3221.924560546875 loss: 7477.0791015625\n",
      "Train Epoch 0: Batch 5180 content: 9515.4287109375 style: 2722.244140625 loss: 9787.6533203125\n",
      "Train Epoch 0: Batch 5200 content: 7493.650390625 style: 2369.485107421875 loss: 7730.59912109375\n",
      "Train Epoch 0: Batch 5220 content: 8473.8466796875 style: 2882.961669921875 loss: 8762.142578125\n",
      "Train Epoch 0: Batch 5240 content: 12372.1875 style: 4195.90478515625 loss: 12791.7783203125\n",
      "Train Epoch 0: Batch 5260 content: 20598.0390625 style: 4274.26123046875 loss: 21025.46484375\n",
      "Train Epoch 0: Batch 5280 content: 5855.865234375 style: 2857.195068359375 loss: 6141.5849609375\n",
      "Train Epoch 0: Batch 5300 content: 6114.3046875 style: 3597.919677734375 loss: 6474.0966796875\n",
      "Train Epoch 0: Batch 5320 content: 6679.3203125 style: 3724.41015625 loss: 7051.76123046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Batch 5340 content: 12214.7470703125 style: 3819.072509765625 loss: 12596.654296875\n",
      "Train Epoch 0: Batch 5360 content: 11811.7099609375 style: 2684.432373046875 loss: 12080.1533203125\n",
      "Train Epoch 0: Batch 5380 content: 6966.396484375 style: 2668.526123046875 loss: 7233.2490234375\n",
      "Train Epoch 0: Batch 5400 content: 230766.859375 style: 10274.0341796875 loss: 231794.265625\n",
      "Train Epoch 0: Batch 5420 content: 8828.0546875 style: 2505.240966796875 loss: 9078.5791015625\n",
      "Train Epoch 0: Batch 5440 content: 5934.45361328125 style: 3320.63623046875 loss: 6266.51708984375\n",
      "Train Epoch 0: Batch 5460 content: 32327.08984375 style: 3721.9609375 loss: 32699.28515625\n",
      "Train Epoch 0: Batch 5480 content: 6954.75244140625 style: 2531.1591796875 loss: 7207.8681640625\n",
      "Train Epoch 0: Batch 5500 content: 7568.97900390625 style: 2971.529052734375 loss: 7866.1318359375\n",
      "Train Epoch 0: Batch 5520 content: 8985.888671875 style: 3506.98388671875 loss: 9336.5869140625\n",
      "Train Epoch 0: Batch 5540 content: 10516.7822265625 style: 2472.906982421875 loss: 10764.0732421875\n",
      "Train Epoch 0: Batch 5560 content: 7736.4462890625 style: 3373.620849609375 loss: 8073.80859375\n",
      "Train Epoch 0: Batch 5580 content: 8070.193359375 style: 2808.359375 loss: 8351.029296875\n",
      "Train Epoch 0: Batch 5600 content: 8663.53515625 style: 3235.385498046875 loss: 8987.0732421875\n",
      "Train Epoch 0: Batch 5620 content: 9355.943359375 style: 3797.15673828125 loss: 9735.6591796875\n",
      "Train Epoch 0: Batch 5640 content: 5552.0888671875 style: 1909.681640625 loss: 5743.05712890625\n",
      "Train Epoch 0: Batch 5660 content: 6515.6328125 style: 3851.81689453125 loss: 6900.814453125\n",
      "Train Epoch 0: Batch 5680 content: 7257.9619140625 style: 3316.858642578125 loss: 7589.64794921875\n",
      "Train Epoch 0: Batch 5700 content: 10971.4140625 style: 4277.1142578125 loss: 11399.125\n",
      "Train Epoch 0: Batch 5720 content: 6010.6669921875 style: 2272.576171875 loss: 6237.9248046875\n",
      "Train Epoch 0: Batch 5740 content: 13183.453125 style: 4155.3056640625 loss: 13598.9833984375\n",
      "Train Epoch 0: Batch 5760 content: 6658.736328125 style: 2680.889404296875 loss: 6926.8251953125\n",
      "Train Epoch 0: Batch 5780 content: 10630.376953125 style: 4606.525390625 loss: 11091.029296875\n",
      "Train Epoch 0: Batch 5800 content: 5039.82421875 style: 3221.91455078125 loss: 5362.015625\n",
      "Train Epoch 0: Batch 5820 content: 5418.3837890625 style: 3691.901123046875 loss: 5787.57373046875\n",
      "Train Epoch 0: Batch 5840 content: 30199.69921875 style: 5047.0625 loss: 30704.40625\n",
      "Train Epoch 0: Batch 5860 content: 20299.359375 style: 3542.669677734375 loss: 20653.626953125\n",
      "Train Epoch 0: Batch 5880 content: 6577.53564453125 style: 2263.666748046875 loss: 6803.90234375\n",
      "Train Epoch 0: Batch 5900 content: 8839.125 style: 3143.40625 loss: 9153.4658203125\n",
      "Train Epoch 0: Batch 5920 content: 5903.9482421875 style: 2680.08740234375 loss: 6171.95703125\n",
      "Train Epoch 0: Batch 5940 content: 11745.0615234375 style: 5372.876953125 loss: 12282.349609375\n",
      "Train Epoch 0: Batch 5960 content: 5998.59765625 style: 1870.550537109375 loss: 6185.65283203125\n",
      "Train Epoch 0: Batch 5980 content: 6901.76220703125 style: 3053.882568359375 loss: 7207.150390625\n",
      "Train Epoch 0: Batch 6000 content: 5781.75048828125 style: 2314.120361328125 loss: 6013.16259765625\n",
      "Train Epoch 0: Batch 6020 content: 7985.740234375 style: 3797.196533203125 loss: 8365.4599609375\n",
      "Train Epoch 0: Batch 6040 content: 5820.59716796875 style: 2233.5009765625 loss: 6043.947265625\n",
      "Train Epoch 0: Batch 6060 content: 7267.0380859375 style: 3788.170166015625 loss: 7645.85498046875\n",
      "Train Epoch 0: Batch 6080 content: 21782.658203125 style: 2741.86376953125 loss: 22056.84375\n",
      "Train Epoch 0: Batch 6100 content: 9793.052734375 style: 3326.076904296875 loss: 10125.66015625\n",
      "Train Epoch 0: Batch 6120 content: 7452.8759765625 style: 2651.62890625 loss: 7718.0390625\n",
      "Train Epoch 0: Batch 6140 content: 9393.392578125 style: 4835.8115234375 loss: 9876.9736328125\n",
      "Train Epoch 0: Batch 6160 content: 13552.0302734375 style: 2989.55419921875 loss: 13850.9853515625\n",
      "Train Epoch 0: Batch 6180 content: 5968.5283203125 style: 3208.89453125 loss: 6289.41796875\n",
      "Train Epoch 0: Batch 6200 content: 10288.4375 style: 2535.37451171875 loss: 10541.974609375\n",
      "Train Epoch 0: Batch 6220 content: 5410.31591796875 style: 2199.5732421875 loss: 5630.2734375\n",
      "Train Epoch 0: Batch 6240 content: 5128.33447265625 style: 3656.912841796875 loss: 5494.02587890625\n",
      "Train Epoch 0: Batch 6260 content: 13208.7041015625 style: 2965.0556640625 loss: 13505.2099609375\n",
      "Train Epoch 0: Batch 6280 content: 8010.4228515625 style: 3023.95361328125 loss: 8312.818359375\n",
      "Train Epoch 0: Batch 6300 content: 10671.912109375 style: 3888.85009765625 loss: 11060.796875\n",
      "Train Epoch 0: Batch 6320 content: 15236.009765625 style: 3308.903076171875 loss: 15566.900390625\n",
      "Train Epoch 0: Batch 6340 content: 10911.771484375 style: 2532.040283203125 loss: 11164.9755859375\n",
      "Train Epoch 0: Batch 6360 content: 5094.75146484375 style: 1996.9693603515625 loss: 5294.4482421875\n",
      "Train Epoch 0: Batch 6380 content: 6695.40283203125 style: 2747.87255859375 loss: 6970.18994140625\n",
      "Train Epoch 0: Batch 6400 content: 6651.48876953125 style: 3063.218505859375 loss: 6957.810546875\n",
      "Train Epoch 0: Batch 6420 content: 6310.0458984375 style: 2396.060546875 loss: 6549.65185546875\n",
      "Train Epoch 0: Batch 6440 content: 7482.5390625 style: 2855.6943359375 loss: 7768.1083984375\n",
      "Train Epoch 0: Batch 6460 content: 8558.5771484375 style: 3555.39404296875 loss: 8914.1162109375\n",
      "Train Epoch 0: Batch 6480 content: 19254.904296875 style: 3084.76806640625 loss: 19563.380859375\n",
      "Train Epoch 0: Batch 6500 content: 5515.3779296875 style: 2238.11962890625 loss: 5739.18994140625\n",
      "Train Epoch 0: Batch 6520 content: 7056.6435546875 style: 4291.51953125 loss: 7485.79541015625\n",
      "Train Epoch 0: Batch 6540 content: 6943.64306640625 style: 3128.54248046875 loss: 7256.4970703125\n",
      "Train Epoch 0: Batch 6560 content: 8705.5185546875 style: 2727.482421875 loss: 8978.2666015625\n",
      "Train Epoch 0: Batch 6580 content: 7164.52001953125 style: 3255.068115234375 loss: 7490.02685546875\n",
      "Train Epoch 0: Batch 6600 content: 8101.20458984375 style: 3156.9921875 loss: 8416.904296875\n",
      "Train Epoch 0: Batch 6620 content: 6889.89404296875 style: 2549.304931640625 loss: 7144.82470703125\n",
      "Train Epoch 0: Batch 6640 content: 11086.712890625 style: 2808.65283203125 loss: 11367.578125\n",
      "Train Epoch 0: Batch 6660 content: 7472.078125 style: 2916.246337890625 loss: 7763.70263671875\n",
      "Train Epoch 0: Batch 6680 content: 5560.44189453125 style: 2578.99853515625 loss: 5818.341796875\n",
      "Train Epoch 0: Batch 6700 content: 11238.03515625 style: 2747.242431640625 loss: 11512.759765625\n",
      "Train Epoch 0: Batch 6720 content: 7723.8876953125 style: 2404.732666015625 loss: 7964.36083984375\n",
      "Train Epoch 0: Batch 6740 content: 5704.04541015625 style: 2559.14208984375 loss: 5959.95947265625\n",
      "Train Epoch 0: Batch 6760 content: 7733.46044921875 style: 3619.017333984375 loss: 8095.3623046875\n",
      "Train Epoch 0: Batch 6780 content: 6459.8798828125 style: 2959.739501953125 loss: 6755.85400390625\n",
      "Train Epoch 0: Batch 6800 content: 6556.37451171875 style: 3247.07275390625 loss: 6881.08203125\n",
      "Train Epoch 0: Batch 6820 content: 7269.380859375 style: 3059.49267578125 loss: 7575.330078125\n",
      "Train Epoch 0: Batch 6840 content: 6698.669921875 style: 2707.755126953125 loss: 6969.4453125\n",
      "Train Epoch 0: Batch 6860 content: 18472.580078125 style: 3468.23291015625 loss: 18819.404296875\n",
      "Train Epoch 0: Batch 6880 content: 7257.259765625 style: 2557.55322265625 loss: 7513.01513671875\n",
      "Train Epoch 0: Batch 6900 content: 6055.44921875 style: 2704.823974609375 loss: 6325.931640625\n",
      "Train Epoch 0: Batch 6920 content: 5197.4111328125 style: 2022.19091796875 loss: 5399.63037109375\n",
      "Train Epoch 0: Batch 6940 content: 5450.19580078125 style: 4232.59912109375 loss: 5873.45556640625\n",
      "Train Epoch 0: Batch 6960 content: 8861.404296875 style: 3090.288818359375 loss: 9170.43359375\n",
      "Train Epoch 0: Batch 6980 content: 6952.31201171875 style: 2804.98291015625 loss: 7232.810546875\n",
      "Train Epoch 0: Batch 7000 content: 7201.1591796875 style: 4231.91455078125 loss: 7624.3505859375\n",
      "Train Epoch 0: Batch 7020 content: 14432.37890625 style: 5873.11669921875 loss: 15019.6904296875\n",
      "Train Epoch 0: Batch 7040 content: 6517.234375 style: 4428.04443359375 loss: 6960.0390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Batch 7060 content: 8239.7451171875 style: 2398.23193359375 loss: 8479.568359375\n",
      "Train Epoch 0: Batch 7080 content: 9717.7763671875 style: 2571.9453125 loss: 9974.970703125\n",
      "Train Epoch 0: Batch 7100 content: 6130.177734375 style: 2687.767333984375 loss: 6398.95458984375\n",
      "Train Epoch 0: Batch 7120 content: 5466.4912109375 style: 2462.013916015625 loss: 5712.6923828125\n",
      "Train Epoch 0: Batch 7140 content: 5997.20166015625 style: 2860.58837890625 loss: 6283.2607421875\n",
      "Train Epoch 0: Batch 7160 content: 11128.4599609375 style: 2083.491455078125 loss: 11336.8095703125\n",
      "Train Epoch 0: Batch 7180 content: 5861.7236328125 style: 2984.212890625 loss: 6160.14501953125\n",
      "Train Epoch 0: Batch 7200 content: 6111.392578125 style: 3731.345703125 loss: 6484.52734375\n",
      "Train Epoch 0: Batch 7220 content: 4785.69140625 style: 1818.456298828125 loss: 4967.537109375\n",
      "Train Epoch 0: Batch 7240 content: 5574.5380859375 style: 2672.363037109375 loss: 5841.7744140625\n",
      "Train Epoch 0: Batch 7260 content: 11403.8515625 style: 3262.60302734375 loss: 11730.1123046875\n",
      "Train Epoch 0: Batch 7280 content: 5263.453125 style: 2175.464599609375 loss: 5480.99951171875\n",
      "Train Epoch 0: Batch 7300 content: 6966.060546875 style: 2652.23046875 loss: 7231.28369140625\n",
      "Train Epoch 0: Batch 7320 content: 10550.9580078125 style: 3520.877685546875 loss: 10903.0458984375\n",
      "Train Epoch 0: Batch 7340 content: 10435.58984375 style: 3286.0888671875 loss: 10764.19921875\n",
      "Train Epoch 0: Batch 7360 content: 5927.1376953125 style: 3756.7998046875 loss: 6302.81787109375\n",
      "Train Epoch 0: Batch 7380 content: 6925.3017578125 style: 2655.245849609375 loss: 7190.826171875\n",
      "Train Epoch 0: Batch 7400 content: 5076.7744140625 style: 2161.03759765625 loss: 5292.8779296875\n",
      "Train Epoch 0: Batch 7420 content: 10119.041015625 style: 3349.63720703125 loss: 10454.0048828125\n",
      "Train Epoch 0: Batch 7440 content: 7635.5908203125 style: 3548.836181640625 loss: 7990.474609375\n",
      "Train Epoch 0: Batch 7460 content: 4292.294921875 style: 1655.9254150390625 loss: 4457.8876953125\n",
      "Train Epoch 0: Batch 7480 content: 13615.119140625 style: 2409.754150390625 loss: 13856.0947265625\n",
      "Train Epoch 0: Batch 7500 content: 6744.5263671875 style: 4576.64111328125 loss: 7202.1904296875\n",
      "Train Epoch 0: Batch 7520 content: 7775.2197265625 style: 4635.14599609375 loss: 8238.734375\n",
      "Train Epoch 0: Batch 7540 content: 15350.158203125 style: 2852.06884765625 loss: 15635.365234375\n",
      "Train Epoch 0: Batch 7560 content: 5560.71728515625 style: 2304.766357421875 loss: 5791.19384765625\n",
      "Train Epoch 0: Batch 7580 content: 9244.939453125 style: 3376.897705078125 loss: 9582.62890625\n",
      "Train Epoch 0: Batch 7600 content: 6566.0029296875 style: 2580.353515625 loss: 6824.0380859375\n",
      "Train Epoch 0: Batch 7620 content: 11695.66015625 style: 2951.705322265625 loss: 11990.8310546875\n",
      "Train Epoch 0: Batch 7640 content: 5883.662109375 style: 2121.218505859375 loss: 6095.7841796875\n",
      "Train Epoch 0: Batch 7660 content: 5990.23681640625 style: 2859.31884765625 loss: 6276.1689453125\n",
      "Train Epoch 0: Batch 7680 content: 6294.271484375 style: 3767.8623046875 loss: 6671.0576171875\n",
      "Train Epoch 0: Batch 7700 content: 8048.98046875 style: 3074.495361328125 loss: 8356.4296875\n",
      "Train Epoch 0: Batch 7720 content: 5902.76171875 style: 2946.427490234375 loss: 6197.404296875\n",
      "Train Epoch 0: Batch 7740 content: 5561.962890625 style: 3791.138671875 loss: 5941.07666015625\n",
      "Train Epoch 0: Batch 7760 content: 5893.3701171875 style: 2865.39599609375 loss: 6179.90966796875\n",
      "Train Epoch 0: Batch 7780 content: 7517.6025390625 style: 2682.311279296875 loss: 7785.83349609375\n",
      "Train Epoch 0: Batch 7800 content: 10405.044921875 style: 3062.363037109375 loss: 10711.28125\n",
      "Train Epoch 0: Batch 7820 content: 7610.888671875 style: 2902.95263671875 loss: 7901.18408203125\n",
      "Train Epoch 0: Batch 7840 content: 8350.28125 style: 4063.47900390625 loss: 8756.62890625\n",
      "Train Epoch 0: Batch 7860 content: 13066.791015625 style: 2634.2939453125 loss: 13330.220703125\n",
      "Train Epoch 0: Batch 7880 content: 11444.87109375 style: 2787.239990234375 loss: 11723.5947265625\n",
      "Train Epoch 0: Batch 7900 content: 6263.8642578125 style: 2169.612060546875 loss: 6480.82568359375\n",
      "Train Epoch 0: Batch 7920 content: 5942.35009765625 style: 4833.185546875 loss: 6425.66845703125\n",
      "Train Epoch 0: Batch 7940 content: 6211.53662109375 style: 4018.23095703125 loss: 6613.35986328125\n",
      "Train Epoch 0: Batch 7960 content: 9235.1953125 style: 2252.190673828125 loss: 9460.4140625\n",
      "Train Epoch 0: Batch 7980 content: 4913.28662109375 style: 2880.79150390625 loss: 5201.36572265625\n",
      "Train Epoch 0: Batch 8000 content: 9519.1259765625 style: 2968.456298828125 loss: 9815.9716796875\n",
      "Train Epoch 0: Batch 8020 content: 5992.50439453125 style: 2984.5263671875 loss: 6290.95703125\n",
      "Train Epoch 0: Batch 8040 content: 5859.89892578125 style: 2165.43896484375 loss: 6076.44287109375\n",
      "Train Epoch 0: Batch 8060 content: 9010.2890625 style: 3015.041748046875 loss: 9311.79296875\n",
      "Train Epoch 0: Batch 8080 content: 89826.765625 style: 3249.14208984375 loss: 90151.6796875\n",
      "Train Epoch 0: Batch 8100 content: 6749.7451171875 style: 2384.189697265625 loss: 6988.1640625\n",
      "Train Epoch 0: Batch 8120 content: 13020.3037109375 style: 4468.86767578125 loss: 13467.1904296875\n",
      "Train Epoch 0: Batch 8140 content: 6357.6787109375 style: 2436.952880859375 loss: 6601.3740234375\n",
      "Train Epoch 0: Batch 8160 content: 7370.24658203125 style: 2645.510986328125 loss: 7634.7978515625\n",
      "Train Epoch 0: Batch 8180 content: 6299.2197265625 style: 4276.18994140625 loss: 6726.8388671875\n",
      "Train Epoch 0: Batch 8200 content: 4579.05615234375 style: 2307.23974609375 loss: 4809.7802734375\n",
      "Train Epoch 0: Batch 8220 content: 17383.4609375 style: 3125.3095703125 loss: 17695.9921875\n",
      "Train Epoch 0: Batch 8240 content: 5331.39599609375 style: 1967.194091796875 loss: 5528.115234375\n",
      "Train Epoch 0: Batch 8260 content: 5734.994140625 style: 3727.119873046875 loss: 6107.7060546875\n",
      "Train Epoch 0: Batch 8280 content: 5970.833984375 style: 2536.21337890625 loss: 6224.455078125\n",
      "Train Epoch 0: Batch 8300 content: 10553.94921875 style: 2669.363525390625 loss: 10820.8857421875\n",
      "Train Epoch 0: Batch 8320 content: 7383.7529296875 style: 3808.259765625 loss: 7764.5791015625\n",
      "Train Epoch 0: Batch 8340 content: 9379.916015625 style: 2446.3642578125 loss: 9624.552734375\n",
      "Train Epoch 0: Batch 8360 content: 12679.240234375 style: 3473.415771484375 loss: 13026.58203125\n",
      "Train Epoch 0: Batch 8380 content: 5385.587890625 style: 3066.25537109375 loss: 5692.21337890625\n",
      "Train Epoch 0: Batch 8400 content: 5470.779296875 style: 2841.84912109375 loss: 5754.96435546875\n",
      "Train Epoch 0: Batch 8420 content: 7569.17041015625 style: 2849.619140625 loss: 7854.13232421875\n",
      "Train Epoch 0: Batch 8440 content: 4952.8427734375 style: 1888.7633056640625 loss: 5141.71923828125\n",
      "Train Epoch 0: Batch 8460 content: 6020.638671875 style: 3017.026611328125 loss: 6322.34130859375\n",
      "Train Epoch 0: Batch 8480 content: 8234.3359375 style: 2701.677001953125 loss: 8504.50390625\n",
      "Train Epoch 0: Batch 8500 content: 7125.4228515625 style: 2107.617919921875 loss: 7336.1845703125\n",
      "Train Epoch 0: Batch 8520 content: 5092.630859375 style: 1975.215087890625 loss: 5290.15234375\n",
      "Train Epoch 0: Batch 8540 content: 5214.08203125 style: 2166.053955078125 loss: 5430.6875\n",
      "Train Epoch 0: Batch 8560 content: 20385.56640625 style: 2322.06640625 loss: 20617.7734375\n",
      "Train Epoch 0: Batch 8580 content: 6055.109375 style: 2629.102783203125 loss: 6318.01953125\n",
      "Train Epoch 0: Batch 8600 content: 17248.517578125 style: 2045.4837646484375 loss: 17453.06640625\n",
      "Train Epoch 0: Batch 8620 content: 13567.01171875 style: 4488.81640625 loss: 14015.8935546875\n",
      "Train Epoch 0: Batch 8640 content: 7730.16845703125 style: 3069.93505859375 loss: 8037.162109375\n",
      "Train Epoch 0: Batch 8660 content: 8090.2373046875 style: 2577.02001953125 loss: 8347.939453125\n",
      "Train Epoch 0: Batch 8680 content: 8751.0771484375 style: 3125.450439453125 loss: 9063.6220703125\n",
      "Train Epoch 0: Batch 8700 content: 9025.4609375 style: 1763.0750732421875 loss: 9201.7685546875\n",
      "Train Epoch 0: Batch 8720 content: 11661.84375 style: 2171.775390625 loss: 11879.021484375\n",
      "Train Epoch 0: Batch 8740 content: 5847.376953125 style: 2088.835205078125 loss: 6056.26025390625\n",
      "Train Epoch 0: Batch 8760 content: 5709.7109375 style: 3784.39599609375 loss: 6088.150390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Batch 8780 content: 7366.98291015625 style: 2428.787841796875 loss: 7609.86181640625\n",
      "Train Epoch 0: Batch 8800 content: 6506.8330078125 style: 3200.636962890625 loss: 6826.896484375\n",
      "Train Epoch 0: Batch 8820 content: 14540.162109375 style: 2095.79736328125 loss: 14749.7421875\n",
      "Train Epoch 0: Batch 8840 content: 6988.84375 style: 2770.24169921875 loss: 7265.8681640625\n",
      "Train Epoch 0: Batch 8860 content: 6679.4150390625 style: 3047.167724609375 loss: 6984.1318359375\n",
      "Train Epoch 0: Batch 8880 content: 6701.53271484375 style: 2317.704345703125 loss: 6933.30322265625\n",
      "Train Epoch 0: Batch 8900 content: 6308.61328125 style: 3637.34130859375 loss: 6672.34765625\n",
      "Train Epoch 0: Batch 8920 content: 25298.658203125 style: 3007.4072265625 loss: 25599.3984375\n",
      "Train Epoch 0: Batch 8940 content: 11810.55078125 style: 3271.254150390625 loss: 12137.67578125\n",
      "Train Epoch 0: Batch 8960 content: 11896.8974609375 style: 2726.836181640625 loss: 12169.5810546875\n",
      "Train Epoch 0: Batch 8980 content: 6607.248046875 style: 4416.36279296875 loss: 7048.88427734375\n",
      "Train Epoch 0: Batch 9000 content: 5978.75048828125 style: 4263.39111328125 loss: 6405.08984375\n",
      "Train Epoch 0: Batch 9020 content: 9560.8662109375 style: 2266.2626953125 loss: 9787.4921875\n",
      "Train Epoch 0: Batch 9040 content: 6790.6240234375 style: 3446.892578125 loss: 7135.3134765625\n",
      "Train Epoch 0: Batch 9060 content: 7499.1259765625 style: 1951.188720703125 loss: 7694.24462890625\n",
      "Train Epoch 0: Batch 9080 content: 6342.34521484375 style: 3052.695068359375 loss: 6647.61474609375\n",
      "Train Epoch 0: Batch 9100 content: 7404.0966796875 style: 2928.68115234375 loss: 7696.96484375\n",
      "Train Epoch 0: Batch 9120 content: 9533.923828125 style: 2242.228759765625 loss: 9758.146484375\n",
      "Train Epoch 0: Batch 9140 content: 9983.3310546875 style: 2043.3602294921875 loss: 10187.6669921875\n",
      "Train Epoch 0: Batch 9160 content: 9154.404296875 style: 2584.84228515625 loss: 9412.888671875\n",
      "Train Epoch 0: Batch 9180 content: 5149.8115234375 style: 2581.40869140625 loss: 5407.9521484375\n",
      "Train Epoch 0: Batch 9200 content: 9100.408203125 style: 3522.948486328125 loss: 9452.703125\n",
      "Train Epoch 0: Batch 9220 content: 13897.3359375 style: 4460.455078125 loss: 14343.3818359375\n",
      "Train Epoch 0: Batch 9240 content: 12263.474609375 style: 2810.45068359375 loss: 12544.51953125\n",
      "Train Epoch 0: Batch 9260 content: 5541.66064453125 style: 2918.0068359375 loss: 5833.46142578125\n",
      "Train Epoch 0: Batch 9280 content: 7349.572265625 style: 2870.299072265625 loss: 7636.60205078125\n",
      "Train Epoch 0: Batch 9300 content: 7278.26171875 style: 2559.700927734375 loss: 7534.23193359375\n",
      "Train Epoch 0: Batch 9320 content: 21000.1640625 style: 3303.4306640625 loss: 21330.5078125\n",
      "Train Epoch 0: Batch 9340 content: 6187.2978515625 style: 2450.484130859375 loss: 6432.34619140625\n",
      "Train Epoch 0: Batch 9360 content: 5423.0302734375 style: 2488.27685546875 loss: 5671.85791015625\n",
      "Train Epoch 0: Batch 9380 content: 10384.3671875 style: 2649.431396484375 loss: 10649.310546875\n",
      "Train Epoch 0: Batch 9400 content: 10576.1123046875 style: 2250.553955078125 loss: 10801.16796875\n",
      "Train Epoch 0: Batch 9420 content: 4891.484375 style: 1816.87353515625 loss: 5073.171875\n",
      "Train Epoch 0: Batch 9440 content: 4844.86962890625 style: 2758.149658203125 loss: 5120.6845703125\n",
      "Train Epoch 0: Batch 9460 content: 9765.01953125 style: 2954.782470703125 loss: 10060.498046875\n",
      "Train Epoch 0: Batch 9480 content: 16363.32421875 style: 3201.14404296875 loss: 16683.439453125\n",
      "Train Epoch 0: Batch 9500 content: 6222.03759765625 style: 2823.792236328125 loss: 6504.4169921875\n",
      "Train Epoch 0: Batch 9520 content: 6009.40673828125 style: 2494.280517578125 loss: 6258.8349609375\n",
      "Train Epoch 0: Batch 9540 content: 8857.267578125 style: 2188.721435546875 loss: 9076.1396484375\n",
      "Train Epoch 0: Batch 9560 content: 6171.6416015625 style: 2762.919189453125 loss: 6447.93359375\n",
      "Train Epoch 0: Batch 9580 content: 15729.01171875 style: 2724.47509765625 loss: 16001.458984375\n",
      "Train Epoch 0: Batch 9600 content: 7782.478515625 style: 2453.531005859375 loss: 8027.83154296875\n",
      "Train Epoch 0: Batch 9620 content: 6881.0166015625 style: 2097.697021484375 loss: 7090.7861328125\n",
      "Train Epoch 0: Batch 9640 content: 11570.193359375 style: 3641.72216796875 loss: 11934.365234375\n",
      "Train Epoch 0: Batch 9660 content: 8976.00390625 style: 2528.806640625 loss: 9228.884765625\n",
      "Train Epoch 0: Batch 9680 content: 7605.23193359375 style: 2596.3212890625 loss: 7864.8642578125\n",
      "Train Epoch 0: Batch 9700 content: 5755.947265625 style: 2588.8408203125 loss: 6014.83154296875\n",
      "Train Epoch 0: Batch 9720 content: 7460.84521484375 style: 2092.280517578125 loss: 7670.0732421875\n",
      "Train Epoch 0: Batch 9740 content: 8096.0244140625 style: 3460.14453125 loss: 8442.0390625\n",
      "Train Epoch 0: Batch 9760 content: 10158.34375 style: 3828.662353515625 loss: 10541.2099609375\n",
      "Train Epoch 0: Batch 9780 content: 4980.00146484375 style: 2203.47119140625 loss: 5200.3486328125\n",
      "Train Epoch 0: Batch 9800 content: 8685.654296875 style: 2751.8740234375 loss: 8960.841796875\n",
      "Train Epoch 0: Batch 9820 content: 5192.66650390625 style: 2397.970703125 loss: 5432.46337890625\n",
      "Train Epoch 0: Batch 9840 content: 5865.98095703125 style: 2079.09130859375 loss: 6073.89013671875\n",
      "Train Epoch 0: Batch 9860 content: 6356.1171875 style: 3736.2236328125 loss: 6729.73974609375\n",
      "Train Epoch 0: Batch 9880 content: 5628.50537109375 style: 2024.18359375 loss: 5830.923828125\n",
      "Train Epoch 0: Batch 9900 content: 9958.15625 style: 2088.782958984375 loss: 10167.0341796875\n",
      "Train Epoch 0: Batch 9920 content: 5332.9873046875 style: 2157.96435546875 loss: 5548.78369140625\n",
      "Train Epoch 1: Batch 0 content: 5917.4833984375 style: 2836.731201171875 loss: 6201.15673828125\n",
      "Train Epoch 1: Batch 20 content: 7498.6318359375 style: 1949.0101318359375 loss: 7693.53271484375\n",
      "Train Epoch 1: Batch 40 content: 11020.8662109375 style: 3698.531982421875 loss: 11390.7197265625\n",
      "Train Epoch 1: Batch 60 content: 5596.48681640625 style: 2063.203125 loss: 5802.80712890625\n",
      "Train Epoch 1: Batch 80 content: 18837.89453125 style: 2764.348388671875 loss: 19114.330078125\n",
      "Train Epoch 1: Batch 100 content: 5940.6787109375 style: 2808.350830078125 loss: 6221.513671875\n",
      "Train Epoch 1: Batch 120 content: 8709.474609375 style: 2793.395263671875 loss: 8988.814453125\n",
      "Train Epoch 1: Batch 140 content: 7268.056640625 style: 2314.929931640625 loss: 7499.5498046875\n",
      "Train Epoch 1: Batch 160 content: 6736.234375 style: 1743.581298828125 loss: 6910.59228515625\n",
      "Train Epoch 1: Batch 180 content: 8941.9501953125 style: 3612.310791015625 loss: 9303.181640625\n",
      "Train Epoch 1: Batch 200 content: 5635.9052734375 style: 2245.445068359375 loss: 5860.44970703125\n",
      "Train Epoch 1: Batch 220 content: 4259.52734375 style: 2015.83935546875 loss: 4461.111328125\n",
      "Train Epoch 1: Batch 240 content: 6974.57763671875 style: 2845.055419921875 loss: 7259.0830078125\n",
      "Train Epoch 1: Batch 260 content: 8886.728515625 style: 2427.385986328125 loss: 9129.466796875\n",
      "Train Epoch 1: Batch 280 content: 6032.2314453125 style: 5716.1181640625 loss: 6603.84326171875\n",
      "Train Epoch 1: Batch 300 content: 5905.44140625 style: 2600.02880859375 loss: 6165.4443359375\n",
      "Train Epoch 1: Batch 320 content: 11145.5029296875 style: 4351.2744140625 loss: 11580.630859375\n",
      "Train Epoch 1: Batch 340 content: 8361.873046875 style: 2125.1435546875 loss: 8574.3876953125\n",
      "Train Epoch 1: Batch 360 content: 5926.939453125 style: 3084.055419921875 loss: 6235.34521484375\n",
      "Train Epoch 1: Batch 380 content: 6244.1611328125 style: 2393.008056640625 loss: 6483.4619140625\n",
      "Train Epoch 1: Batch 400 content: 7005.35986328125 style: 4160.54931640625 loss: 7421.4150390625\n",
      "Train Epoch 1: Batch 420 content: 5470.7275390625 style: 3383.123291015625 loss: 5809.0400390625\n",
      "Train Epoch 1: Batch 440 content: 8000.65234375 style: 1851.0318603515625 loss: 8185.75537109375\n",
      "Train Epoch 1: Batch 460 content: 5859.7890625 style: 2091.71826171875 loss: 6068.9609375\n",
      "Train Epoch 1: Batch 480 content: 6767.19921875 style: 3689.3935546875 loss: 7136.138671875\n",
      "Train Epoch 1: Batch 500 content: 10944.1669921875 style: 2902.2060546875 loss: 11234.3876953125\n",
      "Train Epoch 1: Batch 520 content: 7191.23779296875 style: 2875.9873046875 loss: 7478.83642578125\n",
      "Train Epoch 1: Batch 540 content: 6066.85107421875 style: 2301.6865234375 loss: 6297.01953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: Batch 560 content: 6120.580078125 style: 2444.92236328125 loss: 6365.072265625\n",
      "Train Epoch 1: Batch 580 content: 7778.853515625 style: 2712.29931640625 loss: 8050.08349609375\n",
      "Train Epoch 1: Batch 600 content: 7378.880859375 style: 2379.00048828125 loss: 7616.78076171875\n",
      "Train Epoch 1: Batch 620 content: 8174.96875 style: 2737.95166015625 loss: 8448.763671875\n",
      "Train Epoch 1: Batch 640 content: 6088.14453125 style: 1930.6002197265625 loss: 6281.20458984375\n",
      "Train Epoch 1: Batch 660 content: 10987.361328125 style: 3128.235595703125 loss: 11300.1845703125\n",
      "Train Epoch 1: Batch 680 content: 14094.8671875 style: 3345.682373046875 loss: 14429.435546875\n",
      "Train Epoch 1: Batch 700 content: 24180.15234375 style: 2289.49853515625 loss: 24409.1015625\n",
      "Train Epoch 1: Batch 720 content: 6611.97314453125 style: 2560.990478515625 loss: 6868.072265625\n",
      "Train Epoch 1: Batch 740 content: 6467.884765625 style: 2749.253173828125 loss: 6742.81005859375\n",
      "Train Epoch 1: Batch 760 content: 9399.48046875 style: 1842.292236328125 loss: 9583.7099609375\n",
      "Train Epoch 1: Batch 780 content: 6301.01318359375 style: 2934.845703125 loss: 6594.49755859375\n",
      "Train Epoch 1: Batch 800 content: 28696.470703125 style: 2648.708984375 loss: 28961.341796875\n",
      "Train Epoch 1: Batch 820 content: 6086.47216796875 style: 1936.5050048828125 loss: 6280.12255859375\n",
      "Train Epoch 1: Batch 840 content: 9161.2578125 style: 2780.84423828125 loss: 9439.341796875\n",
      "Train Epoch 1: Batch 860 content: 6253.0263671875 style: 2417.745361328125 loss: 6494.80078125\n",
      "Train Epoch 1: Batch 880 content: 6088.16259765625 style: 1913.450439453125 loss: 6279.5078125\n",
      "Train Epoch 1: Batch 900 content: 5720.30517578125 style: 2086.123291015625 loss: 5928.91748046875\n",
      "Train Epoch 1: Batch 920 content: 6001.23291015625 style: 3782.0380859375 loss: 6379.4365234375\n",
      "Train Epoch 1: Batch 940 content: 7972.00634765625 style: 3402.469970703125 loss: 8312.2529296875\n",
      "Train Epoch 1: Batch 960 content: 11252.125 style: 4275.60693359375 loss: 11679.685546875\n",
      "Train Epoch 1: Batch 980 content: 6339.58984375 style: 2317.609375 loss: 6571.3505859375\n",
      "Train Epoch 1: Batch 1000 content: 12290.3212890625 style: 3347.8525390625 loss: 12625.1064453125\n",
      "Train Epoch 1: Batch 1020 content: 7405.0458984375 style: 2017.3419189453125 loss: 7606.7802734375\n",
      "Train Epoch 1: Batch 1040 content: 33245.9296875 style: 4756.3955078125 loss: 33721.5703125\n",
      "Train Epoch 1: Batch 1060 content: 6154.455078125 style: 2658.03564453125 loss: 6420.2587890625\n",
      "Train Epoch 1: Batch 1080 content: 7086.396484375 style: 2681.56396484375 loss: 7354.552734375\n",
      "Train Epoch 1: Batch 1100 content: 8501.5107421875 style: 2073.88330078125 loss: 8708.8994140625\n",
      "Train Epoch 1: Batch 1120 content: 29386.5859375 style: 5463.142578125 loss: 29932.900390625\n",
      "Train Epoch 1: Batch 1140 content: 6105.8994140625 style: 2684.505615234375 loss: 6374.35009765625\n",
      "Train Epoch 1: Batch 1160 content: 8867.099609375 style: 4498.025390625 loss: 9316.90234375\n",
      "Train Epoch 1: Batch 1180 content: 9486.189453125 style: 2953.873046875 loss: 9781.5771484375\n",
      "Train Epoch 1: Batch 1200 content: 5680.98291015625 style: 2683.787353515625 loss: 5949.36181640625\n",
      "Train Epoch 1: Batch 1220 content: 25890.77734375 style: 2980.963134765625 loss: 26188.873046875\n",
      "Train Epoch 1: Batch 1240 content: 5099.72021484375 style: 2414.309326171875 loss: 5341.1513671875\n",
      "Train Epoch 1: Batch 1260 content: 7383.1220703125 style: 3384.76416015625 loss: 7721.5986328125\n",
      "Train Epoch 1: Batch 1280 content: 8983.154296875 style: 2914.180908203125 loss: 9274.572265625\n",
      "Train Epoch 1: Batch 1300 content: 6716.7060546875 style: 2949.54150390625 loss: 7011.66015625\n",
      "Train Epoch 1: Batch 1320 content: 5239.7412109375 style: 2202.48291015625 loss: 5459.9892578125\n",
      "Train Epoch 1: Batch 1340 content: 10047.2744140625 style: 2609.359130859375 loss: 10308.2099609375\n",
      "Train Epoch 1: Batch 1360 content: 5071.2080078125 style: 2477.59716796875 loss: 5318.9677734375\n",
      "Train Epoch 1: Batch 1380 content: 5181.0419921875 style: 2244.864013671875 loss: 5405.5283203125\n",
      "Train Epoch 1: Batch 1400 content: 5587.47119140625 style: 2289.228271484375 loss: 5816.39404296875\n",
      "Train Epoch 1: Batch 1420 content: 8960.240234375 style: 2822.8994140625 loss: 9242.5302734375\n",
      "Train Epoch 1: Batch 1440 content: 6113.82470703125 style: 2081.501708984375 loss: 6321.97509765625\n",
      "Train Epoch 1: Batch 1460 content: 10285.8916015625 style: 2666.304931640625 loss: 10552.5224609375\n",
      "Train Epoch 1: Batch 1480 content: 5860.0537109375 style: 2446.63671875 loss: 6104.71728515625\n",
      "Train Epoch 1: Batch 1500 content: 8062.8720703125 style: 2075.349609375 loss: 8270.4072265625\n",
      "Train Epoch 1: Batch 1520 content: 7206.20703125 style: 2595.651611328125 loss: 7465.77197265625\n",
      "Train Epoch 1: Batch 1540 content: 6728.513671875 style: 2272.589599609375 loss: 6955.7724609375\n",
      "Train Epoch 1: Batch 1560 content: 5831.48388671875 style: 2890.169189453125 loss: 6120.5009765625\n",
      "Train Epoch 1: Batch 1580 content: 7897.0380859375 style: 2118.717529296875 loss: 8108.90966796875\n",
      "Train Epoch 1: Batch 1600 content: 18392.50390625 style: 2887.631103515625 loss: 18681.267578125\n",
      "Train Epoch 1: Batch 1620 content: 7672.634765625 style: 2433.126953125 loss: 7915.947265625\n",
      "Train Epoch 1: Batch 1640 content: 5202.603515625 style: 2207.24267578125 loss: 5423.32763671875\n",
      "Train Epoch 1: Batch 1660 content: 14204.1533203125 style: 3593.262939453125 loss: 14563.4794921875\n",
      "Train Epoch 1: Batch 1680 content: 7014.96875 style: 2717.990234375 loss: 7286.767578125\n",
      "Train Epoch 1: Batch 1700 content: 5638.35888671875 style: 3258.90673828125 loss: 5964.24951171875\n",
      "Train Epoch 1: Batch 1720 content: 7278.8701171875 style: 2576.290771484375 loss: 7536.4990234375\n",
      "Train Epoch 1: Batch 1740 content: 7676.82470703125 style: 3453.172607421875 loss: 8022.14208984375\n",
      "Train Epoch 1: Batch 1760 content: 8643.0029296875 style: 3394.80029296875 loss: 8982.4833984375\n",
      "Train Epoch 1: Batch 1780 content: 11289.21875 style: 2122.30419921875 loss: 11501.44921875\n",
      "Train Epoch 1: Batch 1800 content: 6308.546875 style: 2182.6455078125 loss: 6526.8115234375\n",
      "Train Epoch 1: Batch 1820 content: 6895.7197265625 style: 2537.098876953125 loss: 7149.4296875\n",
      "Train Epoch 1: Batch 1840 content: 14021.0966796875 style: 1889.5712890625 loss: 14210.0537109375\n",
      "Train Epoch 1: Batch 1860 content: 5623.8974609375 style: 2642.686767578125 loss: 5888.166015625\n",
      "Train Epoch 1: Batch 1880 content: 6006.84326171875 style: 2905.99609375 loss: 6297.44287109375\n",
      "Train Epoch 1: Batch 1900 content: 5865.71484375 style: 2814.759033203125 loss: 6147.19091796875\n",
      "Train Epoch 1: Batch 1920 content: 12992.841796875 style: 2719.2958984375 loss: 13264.771484375\n",
      "Train Epoch 1: Batch 1940 content: 5394.5849609375 style: 2038.6385498046875 loss: 5598.44873046875\n",
      "Train Epoch 1: Batch 1960 content: 5714.77099609375 style: 2305.53662109375 loss: 5945.32470703125\n",
      "Train Epoch 1: Batch 1980 content: 8284.427734375 style: 2316.614501953125 loss: 8516.0888671875\n",
      "Train Epoch 1: Batch 2000 content: 5361.58984375 style: 2786.50439453125 loss: 5640.240234375\n",
      "Train Epoch 1: Batch 2020 content: 6229.05322265625 style: 3184.107421875 loss: 6547.4638671875\n",
      "Train Epoch 1: Batch 2040 content: 5195.50830078125 style: 2349.582275390625 loss: 5430.46630859375\n",
      "Train Epoch 1: Batch 2060 content: 6309.951171875 style: 2313.3125 loss: 6541.2822265625\n",
      "Train Epoch 1: Batch 2080 content: 4892.1552734375 style: 2397.53955078125 loss: 5131.9091796875\n",
      "Train Epoch 1: Batch 2100 content: 26935.802734375 style: 7216.515625 loss: 27657.455078125\n",
      "Train Epoch 1: Batch 2120 content: 6249.3662109375 style: 2578.19384765625 loss: 6507.185546875\n",
      "Train Epoch 1: Batch 2140 content: 8631.177734375 style: 4113.775390625 loss: 9042.5556640625\n",
      "Train Epoch 1: Batch 2160 content: 7058.625 style: 2881.2080078125 loss: 7346.74560546875\n",
      "Train Epoch 1: Batch 2180 content: 9199.84375 style: 5380.142578125 loss: 9737.8583984375\n",
      "Train Epoch 1: Batch 2200 content: 6070.1376953125 style: 2071.207763671875 loss: 6277.25830078125\n",
      "Train Epoch 1: Batch 2220 content: 6271.62451171875 style: 2770.31640625 loss: 6548.65625\n",
      "Train Epoch 1: Batch 2240 content: 6057.52783203125 style: 3824.734375 loss: 6440.00146484375\n",
      "Train Epoch 1: Batch 2260 content: 7666.2431640625 style: 2671.12255859375 loss: 7933.35546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: Batch 2280 content: 4971.3154296875 style: 1798.1226806640625 loss: 5151.1279296875\n",
      "Train Epoch 1: Batch 2300 content: 5749.6328125 style: 2371.055908203125 loss: 5986.73828125\n",
      "Train Epoch 1: Batch 2320 content: 7536.30859375 style: 2278.746826171875 loss: 7764.18310546875\n",
      "Train Epoch 1: Batch 2340 content: 8560.6513671875 style: 2511.339599609375 loss: 8811.78515625\n",
      "Train Epoch 1: Batch 2360 content: 11612.8193359375 style: 1948.989990234375 loss: 11807.71875\n",
      "Train Epoch 1: Batch 2380 content: 5244.31396484375 style: 2321.078857421875 loss: 5476.421875\n",
      "Train Epoch 1: Batch 2400 content: 4896.0126953125 style: 2413.448974609375 loss: 5137.357421875\n",
      "Train Epoch 1: Batch 2420 content: 5456.650390625 style: 2247.4794921875 loss: 5681.3984375\n",
      "Train Epoch 1: Batch 2440 content: 8152.2119140625 style: 2621.757080078125 loss: 8414.3876953125\n",
      "Train Epoch 1: Batch 2460 content: 6356.708984375 style: 1830.6964111328125 loss: 6539.77880859375\n",
      "Train Epoch 1: Batch 2480 content: 7850.9599609375 style: 5796.35546875 loss: 8430.595703125\n",
      "Train Epoch 1: Batch 2500 content: 18132.994140625 style: 2240.1005859375 loss: 18357.00390625\n",
      "Train Epoch 1: Batch 2520 content: 8793.6318359375 style: 3109.387939453125 loss: 9104.5703125\n",
      "Train Epoch 1: Batch 2540 content: 5170.60400390625 style: 2185.71630859375 loss: 5389.17578125\n",
      "Train Epoch 1: Batch 2560 content: 5529.3046875 style: 2878.302490234375 loss: 5817.134765625\n",
      "Train Epoch 1: Batch 2580 content: 7534.2080078125 style: 1979.884765625 loss: 7732.1962890625\n",
      "Train Epoch 1: Batch 2600 content: 75058.015625 style: 3758.2265625 loss: 75433.8359375\n",
      "Train Epoch 1: Batch 2620 content: 7544.26953125 style: 2218.253662109375 loss: 7766.0947265625\n",
      "Train Epoch 1: Batch 2640 content: 27166.67578125 style: 3221.7138671875 loss: 27488.84765625\n",
      "Train Epoch 1: Batch 2660 content: 7852.123046875 style: 2223.623779296875 loss: 8074.4853515625\n",
      "Train Epoch 1: Batch 2680 content: 6111.28662109375 style: 1945.4190673828125 loss: 6305.82861328125\n",
      "Train Epoch 1: Batch 2700 content: 5676.6328125 style: 3229.872314453125 loss: 5999.6201171875\n",
      "Train Epoch 1: Batch 2720 content: 7604.65234375 style: 2822.6240234375 loss: 7886.91455078125\n",
      "Train Epoch 1: Batch 2740 content: 6600.39453125 style: 2058.491455078125 loss: 6806.24365234375\n",
      "Train Epoch 1: Batch 2760 content: 8386.4970703125 style: 4265.93310546875 loss: 8813.0908203125\n",
      "Train Epoch 1: Batch 2780 content: 6155.3232421875 style: 2275.04052734375 loss: 6382.8271484375\n",
      "Train Epoch 1: Batch 2800 content: 11285.7314453125 style: 5766.2392578125 loss: 11862.35546875\n",
      "Train Epoch 1: Batch 2820 content: 9138.369140625 style: 3084.431884765625 loss: 9446.8125\n",
      "Train Epoch 1: Batch 2840 content: 7031.037109375 style: 2521.111572265625 loss: 7283.1484375\n",
      "Train Epoch 1: Batch 2860 content: 6166.052734375 style: 2145.00537109375 loss: 6380.55322265625\n",
      "Train Epoch 1: Batch 2880 content: 4844.2373046875 style: 2407.080322265625 loss: 5084.9453125\n",
      "Train Epoch 1: Batch 2900 content: 27404.41796875 style: 3099.912353515625 loss: 27714.41015625\n",
      "Train Epoch 1: Batch 2920 content: 5974.5146484375 style: 3122.46044921875 loss: 6286.7607421875\n",
      "Train Epoch 1: Batch 2940 content: 7486.72607421875 style: 2208.35595703125 loss: 7707.5615234375\n",
      "Train Epoch 1: Batch 2960 content: 5958.1083984375 style: 2642.46533203125 loss: 6222.35498046875\n",
      "Train Epoch 1: Batch 2980 content: 5605.55859375 style: 2484.122314453125 loss: 5853.970703125\n",
      "Train Epoch 1: Batch 3000 content: 7127.03076171875 style: 2868.906005859375 loss: 7413.92138671875\n",
      "Train Epoch 1: Batch 3020 content: 8388.19921875 style: 2456.28076171875 loss: 8633.8271484375\n",
      "Train Epoch 1: Batch 3040 content: 7572.3134765625 style: 2807.716064453125 loss: 7853.0849609375\n",
      "Train Epoch 1: Batch 3060 content: 7056.21875 style: 3263.502197265625 loss: 7382.56884765625\n",
      "Train Epoch 1: Batch 3080 content: 7313.7744140625 style: 2683.05810546875 loss: 7582.080078125\n",
      "Train Epoch 1: Batch 3100 content: 5845.69677734375 style: 2162.69677734375 loss: 6061.96630859375\n",
      "Train Epoch 1: Batch 3120 content: 7593.1875 style: 2117.525634765625 loss: 7804.93994140625\n",
      "Train Epoch 1: Batch 3140 content: 6694.40673828125 style: 2552.3408203125 loss: 6949.640625\n",
      "Train Epoch 1: Batch 3160 content: 7080.94189453125 style: 2511.5126953125 loss: 7332.09326171875\n",
      "Train Epoch 1: Batch 3180 content: 5732.40185546875 style: 3052.970703125 loss: 6037.69873046875\n",
      "Train Epoch 1: Batch 3200 content: 7122.3828125 style: 2850.858642578125 loss: 7407.46875\n",
      "Train Epoch 1: Batch 3220 content: 4919.33349609375 style: 1970.0404052734375 loss: 5116.33740234375\n",
      "Train Epoch 1: Batch 3240 content: 5775.24609375 style: 2647.20703125 loss: 6039.966796875\n",
      "Train Epoch 1: Batch 3260 content: 30843.67578125 style: 2552.645263671875 loss: 31098.939453125\n",
      "Train Epoch 1: Batch 3280 content: 6029.142578125 style: 1423.6849365234375 loss: 6171.51123046875\n",
      "Train Epoch 1: Batch 3300 content: 6796.7705078125 style: 2101.495361328125 loss: 7006.919921875\n",
      "Train Epoch 1: Batch 3320 content: 7258.6708984375 style: 2344.627685546875 loss: 7493.1337890625\n",
      "Train Epoch 1: Batch 3340 content: 6681.4814453125 style: 2682.12548828125 loss: 6949.69384765625\n",
      "Train Epoch 1: Batch 3360 content: 4281.7470703125 style: 1813.241943359375 loss: 4463.0712890625\n",
      "Train Epoch 1: Batch 3380 content: 7395.3681640625 style: 2236.260498046875 loss: 7618.994140625\n",
      "Train Epoch 1: Batch 3400 content: 5908.74609375 style: 2072.923095703125 loss: 6116.03857421875\n",
      "Train Epoch 1: Batch 3420 content: 28704.548828125 style: 3029.440185546875 loss: 29007.4921875\n",
      "Train Epoch 1: Batch 3440 content: 5043.18994140625 style: 2687.140869140625 loss: 5311.90380859375\n",
      "Train Epoch 1: Batch 3460 content: 4744.181640625 style: 2339.02880859375 loss: 4978.08447265625\n",
      "Train Epoch 1: Batch 3480 content: 11443.3583984375 style: 3124.420166015625 loss: 11755.80078125\n",
      "Train Epoch 1: Batch 3500 content: 4476.9794921875 style: 2324.127685546875 loss: 4709.39208984375\n",
      "Train Epoch 1: Batch 3520 content: 6291.2578125 style: 2324.918701171875 loss: 6523.74951171875\n",
      "Train Epoch 1: Batch 3540 content: 5000.27001953125 style: 2122.21240234375 loss: 5212.4912109375\n",
      "Train Epoch 1: Batch 3560 content: 8391.783203125 style: 4048.771240234375 loss: 8796.66015625\n",
      "Train Epoch 1: Batch 3580 content: 5470.80859375 style: 2836.60498046875 loss: 5754.46923828125\n",
      "Train Epoch 1: Batch 3600 content: 4117.767578125 style: 2771.513671875 loss: 4394.9189453125\n",
      "Train Epoch 1: Batch 3620 content: 7940.3212890625 style: 2354.45703125 loss: 8175.76708984375\n",
      "Train Epoch 1: Batch 3640 content: 7062.9345703125 style: 2408.127197265625 loss: 7303.7470703125\n",
      "Train Epoch 1: Batch 3660 content: 19041.548828125 style: 2515.42529296875 loss: 19293.091796875\n",
      "Train Epoch 1: Batch 3680 content: 7692.70947265625 style: 2406.025390625 loss: 7933.31201171875\n",
      "Train Epoch 1: Batch 3700 content: 9904.908203125 style: 2049.740966796875 loss: 10109.8818359375\n",
      "Train Epoch 1: Batch 3720 content: 8339.033203125 style: 3082.539794921875 loss: 8647.287109375\n",
      "Train Epoch 1: Batch 3740 content: 10171.611328125 style: 4002.934326171875 loss: 10571.904296875\n",
      "Train Epoch 1: Batch 3760 content: 6915.1630859375 style: 2224.562744140625 loss: 7137.619140625\n",
      "Train Epoch 1: Batch 3780 content: 7608.10302734375 style: 2552.936767578125 loss: 7863.396484375\n",
      "Train Epoch 1: Batch 3800 content: 4546.65771484375 style: 1726.620361328125 loss: 4719.31982421875\n",
      "Train Epoch 1: Batch 3820 content: 5327.87451171875 style: 2266.6591796875 loss: 5554.54052734375\n",
      "Train Epoch 1: Batch 3840 content: 6956.48291015625 style: 2506.125 loss: 7207.09521484375\n",
      "Train Epoch 1: Batch 3860 content: 5780.53271484375 style: 3696.60302734375 loss: 6150.19287109375\n",
      "Train Epoch 1: Batch 3880 content: 6238.3974609375 style: 2118.306640625 loss: 6450.22802734375\n",
      "Train Epoch 1: Batch 3900 content: 7505.0888671875 style: 4954.9658203125 loss: 8000.58544921875\n",
      "Train Epoch 1: Batch 3920 content: 5466.3125 style: 1812.6014404296875 loss: 5647.57275390625\n",
      "Train Epoch 1: Batch 3940 content: 7772.25146484375 style: 3353.153076171875 loss: 8107.56689453125\n",
      "Train Epoch 1: Batch 3960 content: 5194.62548828125 style: 2412.834228515625 loss: 5435.90869140625\n",
      "Train Epoch 1: Batch 3980 content: 4691.81201171875 style: 1589.592041015625 loss: 4850.77099609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: Batch 4000 content: 5676.89599609375 style: 3565.2109375 loss: 6033.4169921875\n",
      "Train Epoch 1: Batch 4020 content: 5386.900390625 style: 2251.285888671875 loss: 5612.02880859375\n",
      "Train Epoch 1: Batch 4040 content: 20402.83203125 style: 2547.7822265625 loss: 20657.609375\n",
      "Train Epoch 1: Batch 4060 content: 5037.822265625 style: 5362.30322265625 loss: 5574.052734375\n",
      "Train Epoch 1: Batch 4080 content: 5915.9375 style: 2152.7822265625 loss: 6131.2158203125\n",
      "Train Epoch 1: Batch 4100 content: 8277.9970703125 style: 2147.271728515625 loss: 8492.724609375\n",
      "Train Epoch 1: Batch 4120 content: 6269.2919921875 style: 2351.087158203125 loss: 6504.40087890625\n",
      "Train Epoch 1: Batch 4140 content: 7837.3818359375 style: 1980.1854248046875 loss: 8035.400390625\n",
      "Train Epoch 1: Batch 4160 content: 8800.5078125 style: 2841.50439453125 loss: 9084.658203125\n",
      "Train Epoch 1: Batch 4180 content: 4543.396484375 style: 1799.5177001953125 loss: 4723.34814453125\n",
      "Train Epoch 1: Batch 4200 content: 5686.546875 style: 2379.992431640625 loss: 5924.5458984375\n",
      "Train Epoch 1: Batch 4220 content: 8389.2841796875 style: 3035.091796875 loss: 8692.79296875\n",
      "Train Epoch 1: Batch 4240 content: 6121.17041015625 style: 2844.865234375 loss: 6405.65673828125\n",
      "Train Epoch 1: Batch 4260 content: 5322.8671875 style: 2022.823974609375 loss: 5525.1494140625\n",
      "Train Epoch 1: Batch 4280 content: 9556.435546875 style: 2727.30029296875 loss: 9829.166015625\n",
      "Train Epoch 1: Batch 4300 content: 7580.66357421875 style: 1881.349609375 loss: 7768.79833984375\n",
      "Train Epoch 1: Batch 4320 content: 5085.3798828125 style: 2677.349365234375 loss: 5353.11474609375\n",
      "Train Epoch 1: Batch 4340 content: 9117.099609375 style: 1566.091552734375 loss: 9273.708984375\n",
      "Train Epoch 1: Batch 4360 content: 4942.5263671875 style: 2206.822021484375 loss: 5163.20849609375\n",
      "Train Epoch 1: Batch 4380 content: 20696.755859375 style: 1952.885986328125 loss: 20892.044921875\n",
      "Train Epoch 1: Batch 4400 content: 6747.4677734375 style: 1706.7982177734375 loss: 6918.1474609375\n",
      "Train Epoch 1: Batch 4420 content: 19282.51171875 style: 2822.443359375 loss: 19564.755859375\n",
      "Train Epoch 1: Batch 4440 content: 7709.5361328125 style: 2578.123291015625 loss: 7967.3486328125\n",
      "Train Epoch 1: Batch 4460 content: 8494.5419921875 style: 2418.044189453125 loss: 8736.3466796875\n",
      "Train Epoch 1: Batch 4480 content: 5110.83349609375 style: 2976.044677734375 loss: 5408.43798828125\n",
      "Train Epoch 1: Batch 4500 content: 6889.048828125 style: 1866.237060546875 loss: 7075.67236328125\n",
      "Train Epoch 1: Batch 4520 content: 5010.271484375 style: 2513.720703125 loss: 5261.6435546875\n",
      "Train Epoch 1: Batch 4540 content: 5655.421875 style: 3009.76708984375 loss: 5956.3984375\n",
      "Train Epoch 1: Batch 4560 content: 6544.2626953125 style: 3368.392333984375 loss: 6881.10205078125\n",
      "Train Epoch 1: Batch 4580 content: 6398.44140625 style: 1956.5103759765625 loss: 6594.09228515625\n",
      "Train Epoch 1: Batch 4600 content: 4977.123046875 style: 2076.650390625 loss: 5184.7880859375\n",
      "Train Epoch 1: Batch 4620 content: 11498.50390625 style: 2863.775390625 loss: 11784.8818359375\n",
      "Train Epoch 1: Batch 4640 content: 8708.8671875 style: 2148.467041015625 loss: 8923.7138671875\n",
      "Train Epoch 1: Batch 4660 content: 4955.451171875 style: 1916.770751953125 loss: 5147.12841796875\n",
      "Train Epoch 1: Batch 4680 content: 5983.50439453125 style: 2459.79833984375 loss: 6229.484375\n",
      "Train Epoch 1: Batch 4700 content: 6595.13232421875 style: 2851.3837890625 loss: 6880.2705078125\n",
      "Train Epoch 1: Batch 4720 content: 8801.6513671875 style: 3134.345947265625 loss: 9115.0859375\n",
      "Train Epoch 1: Batch 4740 content: 5779.17822265625 style: 1944.4619140625 loss: 5973.62451171875\n",
      "Train Epoch 1: Batch 4760 content: 5588.17626953125 style: 2429.923095703125 loss: 5831.16845703125\n",
      "Train Epoch 1: Batch 4780 content: 5451.134765625 style: 2547.87109375 loss: 5705.921875\n",
      "Train Epoch 1: Batch 4800 content: 5382.3662109375 style: 2640.31689453125 loss: 5646.39794921875\n",
      "Train Epoch 1: Batch 4820 content: 5580.8876953125 style: 2357.0693359375 loss: 5816.5947265625\n",
      "Train Epoch 1: Batch 4840 content: 22278.66015625 style: 4964.92041015625 loss: 22775.15234375\n",
      "Train Epoch 1: Batch 4860 content: 4845.30615234375 style: 2583.77685546875 loss: 5103.68359375\n",
      "Train Epoch 1: Batch 4880 content: 6490.0361328125 style: 2085.35400390625 loss: 6698.5712890625\n",
      "Train Epoch 1: Batch 4900 content: 7106.65576171875 style: 2596.3564453125 loss: 7366.29150390625\n",
      "Train Epoch 1: Batch 4920 content: 6946.4521484375 style: 2051.7529296875 loss: 7151.62744140625\n",
      "Train Epoch 1: Batch 4940 content: 18270.58984375 style: 2245.39208984375 loss: 18495.12890625\n",
      "Train Epoch 1: Batch 4960 content: 6622.169921875 style: 2261.387451171875 loss: 6848.30859375\n",
      "Train Epoch 1: Batch 4980 content: 8024.60693359375 style: 2551.17236328125 loss: 8279.724609375\n",
      "Train Epoch 1: Batch 5000 content: 6597.44287109375 style: 1993.7923583984375 loss: 6796.822265625\n",
      "Train Epoch 1: Batch 5020 content: 10367.869140625 style: 2261.60546875 loss: 10594.029296875\n",
      "Train Epoch 1: Batch 5040 content: 5570.1015625 style: 1958.3671875 loss: 5765.9384765625\n",
      "Train Epoch 1: Batch 5060 content: 4794.42578125 style: 2525.055908203125 loss: 5046.93115234375\n",
      "Train Epoch 1: Batch 5080 content: 6127.24267578125 style: 1658.2354736328125 loss: 6293.06640625\n",
      "Train Epoch 1: Batch 5100 content: 8114.3466796875 style: 2161.669677734375 loss: 8330.513671875\n",
      "Train Epoch 1: Batch 5120 content: 5363.5146484375 style: 2260.31982421875 loss: 5589.546875\n",
      "Train Epoch 1: Batch 5140 content: 6411.3388671875 style: 2801.94287109375 loss: 6691.533203125\n",
      "Train Epoch 1: Batch 5160 content: 6164.5576171875 style: 2532.649169921875 loss: 6417.82275390625\n",
      "Train Epoch 1: Batch 5180 content: 8405.798828125 style: 2691.55517578125 loss: 8674.9541015625\n",
      "Train Epoch 1: Batch 5200 content: 7072.14892578125 style: 1820.37255859375 loss: 7254.18603515625\n",
      "Train Epoch 1: Batch 5220 content: 6731.38427734375 style: 2353.00634765625 loss: 6966.68505859375\n",
      "Train Epoch 1: Batch 5240 content: 7950.48046875 style: 2140.0302734375 loss: 8164.4833984375\n",
      "Train Epoch 1: Batch 5260 content: 5186.8408203125 style: 2153.501953125 loss: 5402.19091796875\n",
      "Train Epoch 1: Batch 5280 content: 12751.578125 style: 3065.475341796875 loss: 13058.1259765625\n",
      "Train Epoch 1: Batch 5300 content: 7125.92822265625 style: 5390.73193359375 loss: 7665.00146484375\n",
      "Train Epoch 1: Batch 5320 content: 35768.0703125 style: 2893.31298828125 loss: 36057.40234375\n",
      "Train Epoch 1: Batch 5340 content: 5233.9443359375 style: 2067.609375 loss: 5440.705078125\n",
      "Train Epoch 1: Batch 5360 content: 6433.123046875 style: 3922.545166015625 loss: 6825.37744140625\n",
      "Train Epoch 1: Batch 5380 content: 5689.4287109375 style: 2065.70166015625 loss: 5895.9990234375\n",
      "Train Epoch 1: Batch 5400 content: 11101.3583984375 style: 2962.364013671875 loss: 11397.5947265625\n",
      "Train Epoch 1: Batch 5420 content: 10055.53125 style: 3002.510498046875 loss: 10355.7822265625\n",
      "Train Epoch 1: Batch 5440 content: 10325.7880859375 style: 3881.635498046875 loss: 10713.951171875\n",
      "Train Epoch 1: Batch 5460 content: 4483.498046875 style: 2843.096435546875 loss: 4767.8076171875\n",
      "Train Epoch 1: Batch 5480 content: 5674.5107421875 style: 2710.724365234375 loss: 5945.5830078125\n",
      "Train Epoch 1: Batch 5500 content: 7046.8876953125 style: 2844.93310546875 loss: 7331.380859375\n",
      "Train Epoch 1: Batch 5520 content: 5459.1962890625 style: 2530.302001953125 loss: 5712.2265625\n",
      "Train Epoch 1: Batch 5540 content: 6452.2861328125 style: 2188.44189453125 loss: 6671.13037109375\n",
      "Train Epoch 1: Batch 5560 content: 4967.583984375 style: 1616.212158203125 loss: 5129.205078125\n",
      "Train Epoch 1: Batch 5580 content: 8067.09912109375 style: 2397.616455078125 loss: 8306.8603515625\n",
      "Train Epoch 1: Batch 5600 content: 14913.6875 style: 2751.719482421875 loss: 15188.859375\n",
      "Train Epoch 1: Batch 5620 content: 9113.810546875 style: 1936.7646484375 loss: 9307.4873046875\n",
      "Train Epoch 1: Batch 5640 content: 5100.921875 style: 1945.917236328125 loss: 5295.513671875\n",
      "Train Epoch 1: Batch 5660 content: 8526.0048828125 style: 3075.5341796875 loss: 8833.55859375\n",
      "Train Epoch 1: Batch 5680 content: 6286.33935546875 style: 2353.63232421875 loss: 6521.70263671875\n",
      "Train Epoch 1: Batch 5700 content: 7504.11328125 style: 3609.754638671875 loss: 7865.0888671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: Batch 5720 content: 7002.3056640625 style: 2739.446044921875 loss: 7276.25048828125\n",
      "Train Epoch 1: Batch 5740 content: 4680.8486328125 style: 1832.9765625 loss: 4864.146484375\n",
      "Train Epoch 1: Batch 5760 content: 10028.423828125 style: 1992.2952880859375 loss: 10227.6533203125\n",
      "Train Epoch 1: Batch 5780 content: 5039.7431640625 style: 2526.271484375 loss: 5292.3701171875\n",
      "Train Epoch 1: Batch 5800 content: 8121.357421875 style: 2419.152099609375 loss: 8363.2724609375\n",
      "Train Epoch 1: Batch 5820 content: 31894.5 style: 2954.033447265625 loss: 32189.904296875\n",
      "Train Epoch 1: Batch 5840 content: 16366.2060546875 style: 2789.759033203125 loss: 16645.181640625\n",
      "Train Epoch 1: Batch 5860 content: 5346.71337890625 style: 3238.38037109375 loss: 5670.55126953125\n",
      "Train Epoch 1: Batch 5880 content: 11314.7412109375 style: 6155.9375 loss: 11930.3349609375\n",
      "Train Epoch 1: Batch 5900 content: 5456.3125 style: 2243.59423828125 loss: 5680.671875\n",
      "Train Epoch 1: Batch 5920 content: 5172.642578125 style: 2289.5576171875 loss: 5401.59814453125\n",
      "Train Epoch 1: Batch 5940 content: 9766.984375 style: 2281.708984375 loss: 9995.1552734375\n",
      "Train Epoch 1: Batch 5960 content: 4042.1845703125 style: 2051.2607421875 loss: 4247.310546875\n",
      "Train Epoch 1: Batch 5980 content: 9859.8701171875 style: 2270.992431640625 loss: 10086.9697265625\n",
      "Train Epoch 1: Batch 6000 content: 15737.3271484375 style: 2477.462646484375 loss: 15985.0732421875\n",
      "Train Epoch 1: Batch 6020 content: 5866.095703125 style: 2754.825439453125 loss: 6141.578125\n",
      "Train Epoch 1: Batch 6040 content: 8432.79296875 style: 2824.24072265625 loss: 8715.216796875\n",
      "Train Epoch 1: Batch 6060 content: 10060.6298828125 style: 2651.97802734375 loss: 10325.828125\n",
      "Train Epoch 1: Batch 6080 content: 5312.9912109375 style: 2951.65673828125 loss: 5608.15673828125\n",
      "Train Epoch 1: Batch 6100 content: 7396.88427734375 style: 2264.8486328125 loss: 7623.369140625\n",
      "Train Epoch 1: Batch 6120 content: 7239.67529296875 style: 2676.563232421875 loss: 7507.33154296875\n",
      "Train Epoch 1: Batch 6140 content: 4719.4931640625 style: 2326.748291015625 loss: 4952.16796875\n",
      "Train Epoch 1: Batch 6160 content: 23355.60546875 style: 2510.602294921875 loss: 23606.666015625\n",
      "Train Epoch 1: Batch 6180 content: 11962.548828125 style: 3562.17138671875 loss: 12318.765625\n",
      "Train Epoch 1: Batch 6200 content: 4756.62109375 style: 1774.151123046875 loss: 4934.0361328125\n",
      "Train Epoch 1: Batch 6220 content: 6625.630859375 style: 3543.201904296875 loss: 6979.951171875\n",
      "Train Epoch 1: Batch 6240 content: 8022.25146484375 style: 2456.065673828125 loss: 8267.8583984375\n",
      "Train Epoch 1: Batch 6260 content: 6732.52001953125 style: 2699.578125 loss: 7002.47802734375\n",
      "Train Epoch 1: Batch 6280 content: 5077.619140625 style: 1933.0489501953125 loss: 5270.923828125\n",
      "Train Epoch 1: Batch 6300 content: 6665.66845703125 style: 2928.843505859375 loss: 6958.552734375\n",
      "Train Epoch 1: Batch 6320 content: 5838.3427734375 style: 1935.6295166015625 loss: 6031.90576171875\n",
      "Train Epoch 1: Batch 6340 content: 5035.03271484375 style: 2331.828369140625 loss: 5268.21533203125\n",
      "Train Epoch 1: Batch 6360 content: 5555.67578125 style: 1996.7166748046875 loss: 5755.34765625\n",
      "Train Epoch 1: Batch 6380 content: 5513.9765625 style: 2527.4140625 loss: 5766.7177734375\n",
      "Train Epoch 1: Batch 6400 content: 6776.88623046875 style: 2642.1513671875 loss: 7041.1015625\n",
      "Train Epoch 1: Batch 6420 content: 4755.49169921875 style: 1859.5914306640625 loss: 4941.45068359375\n",
      "Train Epoch 1: Batch 6440 content: 10803.62890625 style: 3730.296875 loss: 11176.658203125\n",
      "Train Epoch 1: Batch 6460 content: 4456.39111328125 style: 1899.381591796875 loss: 4646.3291015625\n",
      "Train Epoch 1: Batch 6480 content: 6574.60546875 style: 2114.127197265625 loss: 6786.01806640625\n",
      "Train Epoch 1: Batch 6500 content: 4389.7900390625 style: 2861.7392578125 loss: 4675.9638671875\n",
      "Train Epoch 1: Batch 6520 content: 5214.30078125 style: 1732.272216796875 loss: 5387.52783203125\n",
      "Train Epoch 1: Batch 6540 content: 6889.484375 style: 1686.6473388671875 loss: 7058.14892578125\n",
      "Train Epoch 1: Batch 6560 content: 4890.2216796875 style: 1702.543701171875 loss: 5060.47607421875\n",
      "Train Epoch 1: Batch 6580 content: 12077.2353515625 style: 2376.752197265625 loss: 12314.91015625\n",
      "Train Epoch 1: Batch 6600 content: 14306.009765625 style: 2412.680908203125 loss: 14547.2783203125\n",
      "Train Epoch 1: Batch 6620 content: 5010.033203125 style: 1852.71240234375 loss: 5195.3046875\n",
      "Train Epoch 1: Batch 6640 content: 5231.0078125 style: 2028.4796142578125 loss: 5433.85595703125\n",
      "Train Epoch 1: Batch 6660 content: 6302.12353515625 style: 1765.026123046875 loss: 6478.6259765625\n",
      "Train Epoch 1: Batch 6680 content: 4227.4755859375 style: 1526.001953125 loss: 4380.07568359375\n",
      "Train Epoch 1: Batch 6700 content: 9042.9833984375 style: 3660.7900390625 loss: 9409.0625\n",
      "Train Epoch 1: Batch 6720 content: 8884.896484375 style: 2577.59521484375 loss: 9142.65625\n",
      "Train Epoch 1: Batch 6740 content: 8838.75390625 style: 3143.343994140625 loss: 9153.087890625\n",
      "Train Epoch 1: Batch 6760 content: 6693.0771484375 style: 2397.630615234375 loss: 6932.84033203125\n",
      "Train Epoch 1: Batch 6780 content: 6368.0732421875 style: 3248.379150390625 loss: 6692.9111328125\n",
      "Train Epoch 1: Batch 6800 content: 5800.67626953125 style: 2423.2021484375 loss: 6042.99658203125\n",
      "Train Epoch 1: Batch 6820 content: 4452.1337890625 style: 2023.3033447265625 loss: 4654.46435546875\n",
      "Train Epoch 1: Batch 6840 content: 6153.8076171875 style: 2963.521728515625 loss: 6450.15966796875\n",
      "Train Epoch 1: Batch 6860 content: 14185.234375 style: 2497.09814453125 loss: 14434.9443359375\n",
      "Train Epoch 1: Batch 6880 content: 4881.13720703125 style: 2380.170166015625 loss: 5119.154296875\n",
      "Train Epoch 1: Batch 6900 content: 8984.1318359375 style: 3022.556884765625 loss: 9286.3876953125\n",
      "Train Epoch 1: Batch 6920 content: 5107.14453125 style: 2478.22509765625 loss: 5354.966796875\n",
      "Train Epoch 1: Batch 6940 content: 6119.955078125 style: 2522.783935546875 loss: 6372.2333984375\n",
      "Train Epoch 1: Batch 6960 content: 8636.443359375 style: 2717.492919921875 loss: 8908.1923828125\n",
      "Train Epoch 1: Batch 6980 content: 7438.126953125 style: 3152.416748046875 loss: 7753.36865234375\n",
      "Train Epoch 1: Batch 7000 content: 4639.205078125 style: 2599.611328125 loss: 4899.166015625\n",
      "Train Epoch 1: Batch 7020 content: 29686.6953125 style: 2648.419921875 loss: 29951.537109375\n",
      "Train Epoch 1: Batch 7040 content: 10059.2763671875 style: 3112.066162109375 loss: 10370.4833984375\n",
      "Train Epoch 1: Batch 7060 content: 7032.05859375 style: 2873.55517578125 loss: 7319.4140625\n",
      "Train Epoch 1: Batch 7080 content: 4656.05517578125 style: 2460.790771484375 loss: 4902.13427734375\n",
      "Train Epoch 1: Batch 7100 content: 5009.4169921875 style: 2509.943603515625 loss: 5260.4111328125\n",
      "Train Epoch 1: Batch 7120 content: 4863.66796875 style: 3426.394775390625 loss: 5206.3076171875\n",
      "Train Epoch 1: Batch 7140 content: 5401.36669921875 style: 2689.478515625 loss: 5670.314453125\n",
      "Train Epoch 1: Batch 7160 content: 5099.919921875 style: 1734.3896484375 loss: 5273.35888671875\n",
      "Train Epoch 1: Batch 7180 content: 5727.7119140625 style: 3286.653076171875 loss: 6056.37744140625\n",
      "Train Epoch 1: Batch 7200 content: 4547.736328125 style: 1850.1717529296875 loss: 4732.75341796875\n",
      "Train Epoch 1: Batch 7220 content: 10702.435546875 style: 2772.90185546875 loss: 10979.7255859375\n",
      "Train Epoch 1: Batch 7240 content: 10470.154296875 style: 3113.770263671875 loss: 10781.53125\n",
      "Train Epoch 1: Batch 7260 content: 6267.4765625 style: 4389.14794921875 loss: 6706.3916015625\n",
      "Train Epoch 1: Batch 7280 content: 6737.3037109375 style: 1942.443359375 loss: 6931.5478515625\n",
      "Train Epoch 1: Batch 7300 content: 6418.17333984375 style: 2185.72119140625 loss: 6636.74560546875\n",
      "Train Epoch 1: Batch 7320 content: 5135.38916015625 style: 2107.475341796875 loss: 5346.13671875\n",
      "Train Epoch 1: Batch 7340 content: 5094.107421875 style: 2552.75 loss: 5349.38232421875\n",
      "Train Epoch 1: Batch 7360 content: 7239.3359375 style: 2218.4345703125 loss: 7461.17919921875\n",
      "Train Epoch 1: Batch 7380 content: 6550.50927734375 style: 1561.51611328125 loss: 6706.6611328125\n",
      "Train Epoch 1: Batch 7400 content: 6625.2255859375 style: 2454.70654296875 loss: 6870.6962890625\n",
      "Train Epoch 1: Batch 7420 content: 7519.798828125 style: 2626.69091796875 loss: 7782.4677734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: Batch 7440 content: 10017.373046875 style: 2804.03759765625 loss: 10297.7763671875\n",
      "Train Epoch 1: Batch 7460 content: 5328.8779296875 style: 2328.1083984375 loss: 5561.68896484375\n",
      "Train Epoch 1: Batch 7480 content: 6437.89892578125 style: 2995.88037109375 loss: 6737.48681640625\n",
      "Train Epoch 1: Batch 7500 content: 13195.8583984375 style: 1985.6142578125 loss: 13394.419921875\n",
      "Train Epoch 1: Batch 7520 content: 6193.498046875 style: 3690.99755859375 loss: 6562.59765625\n",
      "Train Epoch 1: Batch 7540 content: 9579.4716796875 style: 2462.29150390625 loss: 9825.701171875\n",
      "Train Epoch 1: Batch 7560 content: 8632.3994140625 style: 2506.86865234375 loss: 8883.0859375\n",
      "Train Epoch 1: Batch 7580 content: 4261.6015625 style: 1994.6612548828125 loss: 4461.06787109375\n",
      "Train Epoch 1: Batch 7600 content: 8629.189453125 style: 2100.8486328125 loss: 8839.2744140625\n",
      "Train Epoch 1: Batch 7620 content: 5734.00146484375 style: 1690.09228515625 loss: 5903.0107421875\n",
      "Train Epoch 1: Batch 7640 content: 10574.97265625 style: 2356.697998046875 loss: 10810.642578125\n",
      "Train Epoch 1: Batch 7660 content: 5419.99072265625 style: 2221.42333984375 loss: 5642.1328125\n",
      "Train Epoch 1: Batch 7680 content: 8089.98486328125 style: 2317.62548828125 loss: 8321.7470703125\n",
      "Train Epoch 1: Batch 7700 content: 9757.4453125 style: 2888.28466796875 loss: 10046.2734375\n",
      "Train Epoch 1: Batch 7720 content: 6366.93359375 style: 2688.604248046875 loss: 6635.7939453125\n",
      "Train Epoch 1: Batch 7740 content: 4838.72021484375 style: 2676.926513671875 loss: 5106.4130859375\n",
      "Train Epoch 1: Batch 7760 content: 5892.53125 style: 2098.557861328125 loss: 6102.38720703125\n",
      "Train Epoch 1: Batch 7780 content: 5106.09814453125 style: 2649.096923828125 loss: 5371.0078125\n",
      "Train Epoch 1: Batch 7800 content: 9332.146484375 style: 2544.55126953125 loss: 9586.6015625\n",
      "Train Epoch 1: Batch 7820 content: 8191.5283203125 style: 2021.0762939453125 loss: 8393.6357421875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
